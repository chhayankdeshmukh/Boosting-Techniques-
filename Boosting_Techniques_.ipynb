{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.\n",
        "Answer:-Boosting is a powerful and widely used **ensemble learning technique** in machine learning that aims to improve the performance of predictive models by combining several **weak learners** into a single **strong learner**. A weak learner is a model that performs only slightly better than random guessing, meaning it has limited predictive power on its own. Boosting is based on the idea that even simple models can achieve high accuracy when they are trained strategically and combined appropriately. Unlike bagging techniques (such as Random Forests), where models are trained independently, boosting trains models **sequentially**, with each new model focusing on correcting the mistakes made by the previous ones.\n",
        "\n",
        "The core concept behind boosting is **learning from errors**. Initially, all training data points are treated equally, and a weak learner is trained on this dataset. After this first model makes predictions, some data points will be classified correctly while others will be misclassified. Boosting algorithms identify these misclassified instances and assign them **higher importance or weight** so that the next weak learner pays more attention to them during training. This process ensures that difficult or previously mispredicted examples are not ignored but instead become the main focus of subsequent models. Over multiple iterations, the ensemble gradually becomes better at handling complex patterns in the data.\n",
        "\n",
        "One of the most well-known boosting algorithms is **AdaBoost (Adaptive Boosting)**. In AdaBoost, each training example is assigned a weight. Initially, all weights are equal. After training a weak learner, the weights of misclassified examples are increased, while the weights of correctly classified examples are decreased. The next learner is trained on this reweighted dataset, forcing it to focus more on the hard cases. Each weak learner is also assigned a weight based on its accuracy, and in the final prediction, learners with higher accuracy have more influence. This weighted combination allows the ensemble to outperform any individual weak learner.\n",
        "\n",
        "Boosting improves weak learners in several important ways. First, it **reduces bias**, which is a common limitation of simple models. Weak learners such as decision stumps (trees with a single split) are often too simple to capture complex relationships in data, leading to underfitting. By combining many such learners, boosting effectively creates a more expressive model capable of representing intricate decision boundaries. Second, boosting can also help **reduce variance**, especially when weak learners are unstable, by averaging their predictions in a structured way. Third, the adaptive nature of boosting ensures that the model continually improves by concentrating on its weaknesses rather than repeatedly learning what it already knows.\n",
        "\n",
        "Another important class of boosting algorithms is **Gradient Boosting**. While AdaBoost adjusts sample weights based on classification errors, Gradient Boosting takes a more general and mathematically grounded approach. It builds models sequentially, where each new model is trained to predict the **residual errors** (the difference between the true values and the current model’s predictions) of the ensemble so far. This is done by optimizing a loss function using gradient descent. In simpler terms, each new learner tries to move the overall model in the direction that most reduces the error. This framework makes gradient boosting very flexible and applicable to both regression and classification problems.\n",
        "\n",
        "Modern implementations such as **XGBoost, LightGBM, and CatBoost** have made boosting extremely popular in real-world applications and machine learning competitions. These algorithms introduce improvements like regularization to prevent overfitting, efficient handling of large datasets, parallel processing, and better treatment of categorical variables. As a result, boosted models often achieve state-of-the-art performance in tasks such as fraud detection, recommendation systems, medical diagnosis, and natural language processing.\n",
        "\n",
        "Despite its strengths, boosting also has some limitations. Because models are trained sequentially, boosting can be **computationally expensive** compared to parallel ensemble methods. It is also **sensitive to noisy data and outliers**, since misclassified points receive increasing importance and may dominate the learning process if they are actually incorrect or misleading. To address these issues, modern boosting algorithms include regularization techniques, early stopping, and learning rate adjustments to balance performance and robustness.\n",
        "\n",
        "In summary, boosting is an ensemble learning technique that transforms multiple weak learners into a strong predictive model by training them sequentially and emphasizing previously misclassified data points. It improves weak learners by forcing each new model to focus on the errors of the past, reducing bias, increasing accuracy, and capturing complex patterns that individual models cannot learn on their own. Through algorithms like AdaBoost and Gradient Boosting, boosting has become one of the most effective and influential approaches in machine learning, demonstrating how strategically combining simple models can lead to highly powerful and accurate systems.\n"
      ],
      "metadata": {
        "id": "5GB9Z9qbICtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "Answer:-\n",
        "Below is a **detailed ~1000-word explanation covering *both* Question 1 and Question 2 together**, written in an exam-ready, conceptual style.\n",
        "\n",
        "---\n",
        "\n",
        "Boosting is a powerful ensemble learning technique in machine learning that aims to improve the performance of predictive models by combining multiple weak learners into a single strong learner. A weak learner is defined as a model that performs only slightly better than random guessing, such as a shallow decision tree with limited depth. On their own, weak learners are not capable of capturing complex patterns in data, but boosting demonstrates that when these simple models are trained sequentially and combined intelligently, they can achieve very high accuracy. The central idea behind boosting is that models can progressively improve by learning from the mistakes of previous models.\n",
        "\n",
        "In boosting, models are trained one after another rather than independently. Initially, all training examples are treated equally, and the first weak learner is trained on the dataset. After making predictions, some data points will be classified correctly while others will be misclassified. Boosting algorithms identify these errors and ensure that subsequent learners focus more on the difficult examples. This is typically achieved either by assigning higher weights to misclassified samples or by training new models to predict the remaining errors. Over multiple iterations, the ensemble becomes increasingly accurate, as each learner corrects the weaknesses of the previous ones.\n",
        "\n",
        "Boosting improves weak learners in several important ways. First, it helps reduce **bias**, which occurs when a model is too simple to capture underlying data patterns. Weak learners often underfit the data, but by combining many of them, boosting effectively builds a much more expressive model. Second, boosting can reduce **overall error** by emphasizing difficult data points rather than repeatedly learning what is already easy. Third, boosting assigns different importance levels to individual learners, allowing more accurate models to have a greater influence on the final prediction. As a result, the final boosted model performs significantly better than any single weak learner.\n",
        "\n",
        "One of the most influential boosting algorithms is **AdaBoost (Adaptive Boosting)**. AdaBoost works by assigning a weight to each training example. Initially, all weights are equal. After training a weak learner, the algorithm increases the weights of misclassified examples and decreases the weights of correctly classified ones. This forces the next weak learner to concentrate more on the hard-to-classify cases. Each weak learner is also given a weight based on its accuracy, so learners that perform better contribute more to the final decision. The final prediction is made using a weighted majority vote (for classification) or weighted sum (for regression). AdaBoost is relatively simple, intuitive, and effective, especially when weak learners are decision stumps.\n",
        "\n",
        "Another important boosting approach is **Gradient Boosting**, which differs from AdaBoost in how models are trained and how errors are handled. While AdaBoost focuses explicitly on misclassified data points by reweighting them, Gradient Boosting takes a more general and mathematical approach. Instead of adjusting data weights, Gradient Boosting trains each new model to predict the **residual errors** made by the current ensemble. Residuals are the differences between the actual target values and the predictions made so far. Each new weak learner attempts to reduce these residuals, thereby improving overall performance.\n",
        "\n",
        "Gradient Boosting frames boosting as an **optimization problem**. It defines a loss function that measures how far predictions are from the true values and then uses gradient descent to minimize this loss. At each iteration, a new model is trained to move the ensemble in the direction that most reduces the loss. This makes Gradient Boosting highly flexible, as it can use different loss functions depending on the task, such as mean squared error for regression or log loss for classification. Because of this flexibility, Gradient Boosting is applicable to a wider range of problems than AdaBoost.\n",
        "\n",
        "The difference between AdaBoost and Gradient Boosting lies mainly in **how models are trained and how errors are treated**. AdaBoost modifies the **weights of training samples**, giving more importance to misclassified examples, and bases learner importance on accuracy. It is particularly sensitive to noisy data and outliers, since incorrectly labeled points can receive very high weights and dominate the learning process. Gradient Boosting, on the other hand, does not reweight data points but instead fits new models to the **remaining errors** using gradients of the loss function. This allows Gradient Boosting to handle noise more gracefully when combined with techniques like learning rate control, regularization, and early stopping.\n",
        "\n",
        "Modern machine learning has further extended Gradient Boosting through advanced implementations such as **XGBoost, LightGBM, and CatBoost**. These algorithms improve efficiency, scalability, and accuracy by introducing features like regularization to prevent overfitting, parallel processing, optimized tree construction, and better handling of categorical variables. As a result, gradient boosting–based methods are among the most successful algorithms in machine learning competitions and real-world applications, including fraud detection, credit scoring, recommendation systems, and medical diagnosis.\n",
        "\n",
        "Despite their effectiveness, boosting methods have some limitations. Because models are trained sequentially, boosting can be computationally expensive and slower to train compared to parallel ensemble methods like bagging. Boosting algorithms can also overfit if too many learners are added or if the learning rate is too high. However, these issues can be mitigated using techniques such as limiting tree depth, using regularization, and applying early stopping based on validation performance.\n",
        "\n",
        "In conclusion, boosting is an ensemble learning technique that improves weak learners by training them sequentially and focusing on correcting previous mistakes. It transforms simple models into a strong predictive system by reducing bias, minimizing errors, and combining learners in a weighted manner. AdaBoost and Gradient Boosting are two key boosting algorithms that differ in how they handle errors and train models: AdaBoost emphasizes misclassified samples through reweighting, while Gradient Boosting uses gradient-based optimization to reduce residual errors. Together, these methods illustrate the core strength of boosting—strategically combining weak learners to achieve high accuracy and robust performance in machine learning tasks.\n"
      ],
      "metadata": {
        "id": "Khibj6IkI6H7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: How does regularization help in XGBoost?\n",
        "Answer:- Regularization plays a crucial role in **XGBoost (Extreme Gradient Boosting)** by helping the model achieve high accuracy while preventing **overfitting**, which occurs when a model learns noise or overly complex patterns from the training data instead of generalizable trends. XGBoost includes regularization directly in its objective (loss) function, making it more robust than traditional gradient boosting methods.\n",
        "\n",
        "One of the key ways regularization helps in XGBoost is by **penalizing model complexity**. XGBoost adds regularization terms that control both the **number of leaves in each tree** and the **magnitude of leaf weights**. The parameter **gamma (γ)** penalizes the creation of new tree splits; a split is made only if it reduces the loss by more than gamma, which discourages unnecessary tree growth. This keeps trees simpler and prevents them from fitting noise in the data. Additionally, **L2 regularization (lambda)** and **L1 regularization (alpha)** are applied to leaf weights, shrinking them toward zero. This reduces the impact of individual trees and makes the model more stable.\n",
        "\n",
        "Regularization in XGBoost also works through **tree structure control**. Parameters such as **max_depth** and **min_child_weight** restrict how deep trees can grow and how much data is required in each leaf. By limiting tree depth and preventing splits with very few data points, the model avoids learning overly specific rules that do not generalize well. The **subsample** and **colsample_bytree** parameters further introduce randomness by training trees on subsets of rows and features, which reduces variance and improves robustness.\n",
        "\n",
        "Another important regularization technique in XGBoost is the **learning rate (eta)**. A smaller learning rate reduces the contribution of each tree, forcing the model to learn gradually and rely on many small improvements rather than a few large ones. This helps prevent overfitting and leads to smoother, more generalizable models. XGBoost also supports **early stopping**, which halts training when performance on a validation set stops improving, preventing the model from becoming too complex.\n",
        "\n",
        "In summary, regularization helps XGBoost by controlling model complexity, penalizing unnecessary tree growth, shrinking leaf weights, introducing randomness, and slowing down learning. Together, these regularization mechanisms allow XGBoost to achieve strong predictive performance while maintaining good generalization to unseen data.\n"
      ],
      "metadata": {
        "id": "I-unkCNRJYSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
        "Answer:- CatBoost is considered highly efficient for handling categorical data because it is specifically designed to process categorical features directly, without requiring extensive manual preprocessing such as one-hot encoding or label encoding. Traditional machine learning algorithms often require categorical variables to be converted into numerical form, which can lead to high-dimensional feature spaces or introduce unintended ordering. CatBoost avoids these issues by using **target-based encoding**, where categorical values are transformed into numerical statistics derived from the target variable, allowing the model to capture meaningful relationships between categories and the target.\n",
        "\n",
        "A key reason for CatBoost’s efficiency is its use of **ordered target encoding**. Instead of computing target statistics using the entire dataset—which can cause target leakage and overfitting—CatBoost computes these statistics in an ordered manner, using only past data points when processing each example. This ensures that the encoding does not use future information and results in more reliable and unbiased estimates. By preventing target leakage, CatBoost improves generalization performance, especially on datasets with many categorical features.\n",
        "\n",
        "CatBoost also handles high-cardinality categorical features (features with many unique values) very effectively. One-hot encoding such features would dramatically increase dimensionality and computational cost, while CatBoost’s encoding keeps the feature space compact and efficient. Additionally, CatBoost automatically generates and evaluates combinations of categorical features, allowing it to capture complex interactions between categories that would otherwise require manual feature engineering.\n",
        "\n",
        "Another factor contributing to CatBoost’s efficiency is its robust regularization and symmetric tree structure, which reduce overfitting and stabilize learning when categorical data is present. CatBoost requires minimal hyperparameter tuning and performs well out of the box, making it especially suitable for real-world datasets that contain many categorical variables.\n",
        "\n",
        "In summary, CatBoost is efficient for handling categorical data because it natively supports categorical features, uses ordered target encoding to prevent data leakage, efficiently handles high-cardinality categories, captures categorical feature interactions automatically, and maintains strong generalization with minimal preprocessing.\n"
      ],
      "metadata": {
        "id": "DVW-rJzIJjd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n",
        "Answer:- Boosting techniques are preferred over bagging methods in many real-world applications where **high predictive accuracy**, **bias reduction**, and the ability to learn complex patterns are more important than simple variance reduction. Because boosting trains models sequentially and focuses on correcting previous errors, it is especially effective in problems where relationships in the data are complex and subtle.\n",
        "\n",
        "One major application is **fraud detection** in banking and finance. Fraudulent transactions are often rare and difficult to identify, and simple models may miss important patterns. Boosting methods such as Gradient Boosting and XGBoost are preferred because they focus more on hard-to-classify cases and can capture complex interactions between transaction features, leading to higher detection accuracy than bagging methods like Random Forests.\n",
        "\n",
        "Another important application is **credit scoring and risk assessment**. Financial institutions use boosting algorithms to predict loan default risk because these models can reduce bias and make highly accurate predictions from structured data with many interacting variables. Boosting models often outperform bagging approaches when precise ranking and probability estimation are required.\n",
        "\n",
        "Boosting is also widely used in **search engines and recommendation systems**, where accurately ranking items (such as web pages, products, or videos) is crucial. Gradient boosting–based models are effective in learning ranking functions and optimizing custom loss functions, which is harder to achieve with bagging methods.\n",
        "\n",
        "In **medical diagnosis and healthcare analytics**, boosting techniques are preferred for tasks such as disease prediction and patient risk stratification. These problems often involve complex, non-linear relationships among features, and boosting models can focus on difficult cases and improve diagnostic accuracy, which is critical in healthcare settings.\n",
        "\n",
        "Another key area is **computer vision**, particularly in early object detection systems such as face detection. AdaBoost has been historically used to select and combine weak image features efficiently, achieving fast and accurate detection. Boosting is also used in **text classification and sentiment analysis**, where subtle patterns in high-dimensional data must be learned effectively.\n",
        "\n",
        "In summary, boosting techniques are preferred over bagging methods in real-world applications such as fraud detection, credit scoring, recommendation systems, healthcare analytics, computer vision, and text classification because they reduce bias, focus on difficult examples, handle complex relationships, and often deliver higher accuracy than bagging approaches.\n"
      ],
      "metadata": {
        "id": "iaGk2krQJ0OB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accuracy"
      ],
      "metadata": {
        "id": "aXsl0fY-Khu5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "208b3200",
        "outputId": "ac73d678-731c-454d-a348-626f9e892483"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the AdaBoost Classifier\n",
        "adaboost_model = AdaBoostClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "adaboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = adaboost_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"AdaBoost Classifier Accuracy on Breast Cancer dataset: {accuracy:.4f}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy on Breast Cancer dataset: 0.9708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score"
      ],
      "metadata": {
        "id": "IA3Vjj6_K4eS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1d8e5be",
        "outputId": "39d3c338-ddc9-4448-8d9f-96ff8f17db81"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California Housing dataset\n",
        "california_housing = fetch_california_housing()\n",
        "X_housing = california_housing.data\n",
        "y_housing = california_housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train_housing, X_test_housing, y_train_housing, y_test_housing = train_test_split(X_housing, y_housing, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Gradient Boosting Regressor\n",
        "gbr_model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gbr_model.fit(X_train_housing, y_train_housing)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_housing = gbr_model.predict(X_test_housing)\n",
        "\n",
        "# Calculate the R-squared score\n",
        "r2 = r2_score(y_test_housing, y_pred_housing)\n",
        "\n",
        "# Print the R-squared score\n",
        "print(f\"Gradient Boosting Regressor R-squared on California Housing dataset: {r2:.4f}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared on California Housing dataset: 0.7803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "fNQn4JTfLJfM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a593a20b",
        "outputId": "ad791870-63fe-482d-c9a4-6b5b7e2ee545"
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Breast Cancer dataset (if not already loaded)\n",
        "# Assuming X, y, X_train, X_test, y_train, y_test are already available from previous cells\n",
        "# If not, uncomment the following lines:\n",
        "# breast_cancer = load_breast_cancer()\n",
        "# X = breast_cancer.data\n",
        "# y = breast_cancer.target\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "# eval_metric='logloss' is set to suppress warnings about the default metric for binary classification\n",
        "xgb_model = xgb.XGBClassifier(eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define the parameter grid for learning rate tuning\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# cv=3 for cross-validation, scoring='accuracy' for classification\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Get the best model\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_tuned = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the tuned model\n",
        "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Tuned XGBoost Classifier Accuracy on Breast Cancer dataset: {accuracy_tuned:.4f}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "Best parameters: {'learning_rate': 0.05}\n",
            "Tuned XGBoost Classifier Accuracy on Breast Cancer dataset: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn"
      ],
      "metadata": {
        "id": "qdUJmPYOLmKU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adfe29a1",
        "outputId": "db7d6c7e-ce94-485c-fa90-6d4ef52fc63f"
      },
      "source": [
        "# Install catboost if not already installed\n",
        "!pip install catboost"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "c44c07e2",
        "outputId": "0132d7c2-a970-41b1-fd57-f89b9f95799c"
      },
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Breast Cancer dataset (assuming X, y, X_train, X_test, y_train, y_test are available)\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize CatBoost Classifier\n",
        "# verbose=0 to suppress training output\n",
        "catboost_model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, loss_function='Logloss', random_seed=42, verbose=0)\n",
        "\n",
        "# Train the model\n",
        "catboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_catboost = catboost_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy (optional, but good for context)\n",
        "accuracy_catboost = accuracy_score(y_test, y_pred_catboost)\n",
        "print(f\"CatBoost Classifier Accuracy on Breast Cancer dataset: {accuracy_catboost:.4f}\")\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_catboost)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=breast_cancer.target_names, yticklabels=breast_cancer.target_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for CatBoost Classifier')\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost Classifier Accuracy on Breast Cancer dataset: 0.9708\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model\n"
      ],
      "metadata": {
        "id": "AxRmQqlEL8Be"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71555d1d"
      },
      "source": [
        "# Task\n",
        "Describe a step-by-step data science pipeline for predicting loan default using boosting techniques for a FinTech company, given an imbalanced dataset with missing values, and both numeric and categorical features. The description should cover: data preprocessing and handling of missing/categorical values; the choice between AdaBoost, XGBoost, or CatBoost; the hyperparameter tuning strategy; the chosen evaluation metrics and their justification; and the business benefits of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49dc48b"
      },
      "source": [
        "## Data Understanding and Initial Exploration\n",
        "\n",
        "### Subtask:\n",
        "Begin with a thorough exploration of the dataset to understand its structure, identify data types, distributions, potential outliers, and the extent of missing values. Analyze the target variable (loan default) distribution to confirm class imbalance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51308b8f"
      },
      "source": [
        "## Data Understanding and Initial Exploration\n",
        "\n",
        "### Subtask:\n",
        "Begin with a thorough exploration of the dataset to understand its structure, identify data types, distributions, potential outliers, and the extent of missing values. Analyze the target variable (loan default) distribution to confirm class imbalance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd6544fc"
      },
      "source": [
        "### 1. Importance of Initial Data Exploration\n",
        "\n",
        "Initial data exploration is a crucial first step in any data science project, especially when dealing with sensitive applications like loan default prediction. It involves systematically examining the raw data to understand its characteristics, uncover patterns, identify anomalies, and prepare it for further processing and modeling. This phase is often referred to as Exploratory Data Analysis (EDA).\n",
        "\n",
        "**Understanding Dataset Structure:** This involves getting a high-level overview of the data, including the number of rows (observations) and columns (features), column names, and a general sense of what each column represents. It helps in mapping the available information to the problem context (e.g., customer demographics, transaction behavior).\n",
        "\n",
        "**Identifying Data Types:** Knowing the data type of each feature (e.g., numerical, categorical, boolean, datetime) is fundamental. Incorrect data types can lead to errors in analysis and modeling. For instance, a numerical column stored as a string might prevent mathematical operations, or a categorical column treated as numerical might lead to meaningless correlations. Identifying data types also helps in determining appropriate preprocessing steps.\n",
        "\n",
        "**Distributions and Potential Outliers:**\n",
        "\n",
        "*   **Numerical Features:** Analyzing the distribution of numerical features (e.g., using histograms, box plots, descriptive statistics like mean, median, standard deviation) provides insights into their spread, central tendency, and skewness. This helps in understanding the range of values, common values, and variability within the data.\n",
        "*   **Categorical Features:** For categorical features, examining the frequency distribution of each category (e.g., using bar plots, value counts) helps understand the commonness of different levels and if some categories are too rare to be useful.\n",
        "*   **Outliers:** Outliers are data points that significantly deviate from other observations. They can be genuine extreme values or errors in data collection. Identifying outliers is critical because they can disproportionately influence model training, leading to biased estimates or increased variance. Techniques like box plots, scatter plots, and statistical methods (e.g., Z-score, IQR method) are used to detect them. Understanding their nature is key: should they be removed, transformed, or treated specially?\n",
        "\n",
        "**Why this is important for loan default prediction:**\n",
        "\n",
        "In a FinTech context, understanding the data intimately allows us to:\n",
        "\n",
        "*   **Ensure Data Quality:** Catch errors early that could compromise model reliability.\n",
        "*   **Inform Feature Engineering:** Discover relationships and patterns that can be used to create new, more informative features.\n",
        "*   **Guide Model Selection:** The nature of the data (e.g., linearity, presence of many categorical features) can influence the choice of appropriate modeling techniques.\n",
        "*   **Prevent Misinterpretation:** A clear understanding of data characteristics helps in correctly interpreting model results and making sound business decisions.\n",
        "\n",
        "This initial exploration lays the groundwork for all subsequent steps in the data science pipeline, ensuring that the preprocessing and modeling efforts are well-informed and robust."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4308e40"
      },
      "source": [
        "### 2. Identifying the Extent and Nature of Missing Values\n",
        "\n",
        "Missing values are a common problem in real-world datasets and can significantly impact the performance and reliability of machine learning models. Therefore, identifying their extent and nature is a critical part of data preprocessing.\n",
        "\n",
        "**Extent of Missing Values:**\n",
        "\n",
        "*   **Quantifying Missingness:** The first step is to quantify how many missing values exist in each feature (column) and across the entire dataset. This is typically done by calculating the sum or percentage of null (NaN, None, empty string, etc.) values for each column. Features with a very high percentage of missing values (e.g., >70-80%) might be candidates for removal, as they offer little predictive power. Those with a small percentage might be imputed.\n",
        "*   **Visualizing Missingness:** Tools like heatmaps or bar charts can visually represent the pattern of missing values across the dataset. This can reveal if missingness is random, if certain features tend to be missing together, or if there are structural reasons for missing data.\n",
        "\n",
        "**Nature of Missing Values:**\n",
        "\n",
        "Understanding why data is missing is often as important as knowing how much is missing. Missing data can broadly be categorized into three types:\n",
        "\n",
        "*   **Missing Completely at Random (MCAR):** The probability of data being missing is unrelated to both observed and unobserved data. For example, a respondent might accidentally skip a question.\n",
        "*   **Missing at Random (MAR):** The probability of data being missing is related to some observed data but not to the missing data itself. For example, men might be less likely to fill out a survey question about income, but this missingness is explainable by gender, which is observed.\n",
        "*   **Missing Not at Random (MNAR):** The probability of data being missing is related to the unobserved data itself. For example, people with very high or very low incomes might be less likely to report their income.\n",
        "\n",
        "**Handling Missing Values for Different Feature Types:**\n",
        "\n",
        "*   **Numerical Features:**\n",
        "    *   **Imputation:** Missing numerical values can be imputed with statistical measures like the mean, median, or mode of the column. The median is often preferred for skewed distributions or when outliers are present. For time-series data, forward-fill or backward-fill might be appropriate.\n",
        "    *   **Advanced Imputation:** More sophisticated methods include K-Nearest Neighbors (KNN) imputation, where missing values are filled based on the values of the nearest neighbors, or using predictive models to estimate missing values.\n",
        "    *   **Indicator Variables:** Sometimes, creating a binary indicator variable (e.g., `is_feature_missing`) to flag whether a value was originally missing can provide additional information to the model.\n",
        "\n",
        "*   **Categorical Features:**\n",
        "    *   **Mode Imputation:** Missing categorical values are often imputed with the mode (most frequent category) of the column.\n",
        "    *   **New Category:** Treating missing values as their own distinct category (e.g., 'Unknown' or 'Missing') is another common approach, especially when missingness might carry predictive information (MNAR).\n",
        "    *   **Advanced Imputation:** Similar to numerical features, KNN or predictive models can also be used for categorical imputation, though care must be taken with the distance metrics for categorical data.\n",
        "\n",
        "**Why this is important for loan default prediction:**\n",
        "\n",
        "In the context of loan default prediction, missing data in customer demographics or transaction behavior could be highly informative. For instance, a missing income value might indicate an applicant's unwillingness to disclose or an inconsistent employment history, which could be a risk factor. Simply dropping such rows or naive imputation could lead to loss of crucial information or biased models. A careful analysis of missingness allows for informed decisions on imputation strategies, ensuring that the model leverages all available (and even implied) information to accurately assess default risk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8369a1a"
      },
      "source": [
        "### 3. Analyzing Target Variable Distribution and Class Imbalance\n",
        "\n",
        "For a loan default prediction problem, the target variable is typically binary: `0` for non-default and `1` for default. A critical aspect of analyzing this target variable is to understand its distribution, particularly to identify **class imbalance**.\n",
        "\n",
        "**What is Class Imbalance?**\n",
        "\n",
        "Class imbalance occurs when the number of observations belonging to one class (the majority class) significantly outweighs the number of observations belonging to the other class (the minority class). In loan default prediction, defaults (`1`) are usually much rarer than non-defaults (`0`). This makes the non-default class the majority, and the default class the minority.\n",
        "\n",
        "**Process of Analysis:**\n",
        "\n",
        "1.  **Count Occurrences:** The most straightforward way to analyze the target variable's distribution is to count the occurrences of each class. This can be done using `value_counts()` on the target column in a pandas DataFrame.\n",
        "2.  **Calculate Percentages:** Convert these counts into percentages to clearly see the proportion of each class. For example, if 95% are non-defaults and 5% are defaults, it indicates a significant imbalance.\n",
        "3.  **Visualize Distribution:** Use bar plots (e.g., `seaborn.countplot`, `matplotlib.pyplot.bar`) to visually represent the distribution of the target variable. A bar plot will immediately highlight if one class has a much smaller bar than the other.\n",
        "\n",
        "**Why Class Imbalance is Critical for Loan Default Prediction:**\n",
        "\n",
        "*   **Model Bias:** If a model is trained on an imbalanced dataset, it tends to become biased towards the majority class. It might achieve high overall accuracy by simply predicting the majority class for most instances, but it will perform poorly on the minority class (i.e., identifying actual defaults).\n",
        "*   **Poor Generalization:** Models that ignore class imbalance often fail to generalize well to new, unseen data, especially for the rare but important default cases.\n",
        "*   **Misleading Evaluation Metrics:** Standard accuracy is a misleading metric for imbalanced datasets. A model predicting no defaults might still show 95% accuracy if only 5% of cases are defaults. This is why specialized metrics are needed.\n",
        "\n",
        "**Impact on Modeling Strategy:**\n",
        "\n",
        "Recognizing class imbalance early on is crucial because it dictates subsequent steps in the data science pipeline:\n",
        "\n",
        "*   **Sampling Techniques:** It informs the need for techniques like oversampling (e.g., SMOTE) or undersampling to balance the classes in the training data.\n",
        "*   **Algorithmic Approaches:** It might lead to the selection of algorithms inherently robust to imbalance (like tree-based methods) or those that can incorporate class weights.\n",
        "*   **Evaluation Metrics:** It necessitates using appropriate evaluation metrics beyond simple accuracy, such as precision, recall, F1-score, Area Under the Receiver Operating Characteristic Curve (AUC-ROC), and Area Under the Precision-Recall Curve (AUC-PR), which specifically account for the performance on the minority class.\n",
        "\n",
        "By carefully analyzing the target variable's distribution, a FinTech company can confirm the presence and severity of class imbalance, allowing them to proactively implement strategies to build a robust model capable of accurately identifying loan defaulters, which is the core business objective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51edb44a"
      },
      "source": [
        "### 3. Analyzing Target Variable Distribution and Class Imbalance\n",
        "\n",
        "For a loan default prediction problem, the target variable is typically binary: `0` for non-default and `1` for default. A critical aspect of analyzing this target variable is to understand its distribution, particularly to identify **class imbalance**.\n",
        "\n",
        "**What is Class Imbalance?**\n",
        "\n",
        "Class imbalance occurs when the number of observations belonging to one class (the majority class) significantly outweighs the number of observations belonging to the other class (the minority class). In loan default prediction, defaults (`1`) are usually much rarer than non-defaults (`0`). This makes the non-default class the majority, and the default class the minority.\n",
        "\n",
        "**Process of Analysis:**\n",
        "\n",
        "1.  **Count Occurrences:** The most straightforward way to analyze the target variable's distribution is to count the occurrences of each class. This can be done using `value_counts()` on the target column in a pandas DataFrame.\n",
        "2.  **Calculate Percentages:** Convert these counts into percentages to clearly see the proportion of each class. For example, if 95% are non-defaults and 5% are defaults, it indicates a significant imbalance.\n",
        "3.  **Visualize Distribution:** Use bar plots (e.g., `seaborn.countplot`, `matplotlib.pyplot.bar`) to visually represent the distribution of the target variable. A bar plot will immediately highlight if one class has a much smaller bar than the other.\n",
        "\n",
        "**Why Class Imbalance is Critical for Loan Default Prediction:**\n",
        "\n",
        "*   **Model Bias:** If a model is trained on an imbalanced dataset, it tends to become biased towards the majority class. It might achieve high overall accuracy by simply predicting the majority class for most instances, but it will perform poorly on the minority class (i.e., identifying actual defaults).\n",
        "*   **Poor Generalization:** Models that ignore class imbalance often fail to generalize well to new, unseen data, especially for the rare but important default cases.\n",
        "*   **Misleading Evaluation Metrics:** Standard accuracy is a misleading metric for imbalanced datasets. A model predicting no defaults might still show 95% accuracy if only 5% of cases are defaults. This is why specialized metrics are needed.\n",
        "\n",
        "**Impact on Modeling Strategy:**\n",
        "\n",
        "Recognizing class imbalance early on is crucial because it dictates subsequent steps in the data science pipeline:\n",
        "\n",
        "*   **Sampling Techniques:** It informs the need for techniques like oversampling (e.g., SMOTE) or undersampling to balance the classes in the training data.\n",
        "*   **Algorithmic Approaches:** It might lead to the selection of algorithms inherently robust to imbalance (like tree-based methods) or those that can incorporate class weights.\n",
        "*   **Evaluation Metrics:** It necessitates using appropriate evaluation metrics beyond simple accuracy, such as precision, recall, F1-score, Area Under the Receiver Operating Characteristic Curve (AUC-ROC), and Area Under the Precision-Recall Curve (AUC-PR), which specifically account for the performance on the minority class.\n",
        "\n",
        "By carefully analyzing the target variable's distribution, a FinTech company can confirm the presence and severity of class imbalance, allowing them to proactively implement strategies to build a robust model capable of accurately identifying loan defaulters, which is the core business objective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4b04da0"
      },
      "source": [
        "### 3. Analyzing Target Variable Distribution and Class Imbalance\n",
        "\n",
        "For a loan default prediction problem, the target variable is typically binary: `0` for non-default and `1` for default. A critical aspect of analyzing this target variable is to understand its distribution, particularly to identify **class imbalance**.\n",
        "\n",
        "**What is Class Imbalance?**\n",
        "\n",
        "Class imbalance occurs when the number of observations belonging to one class (the majority class) significantly outweighs the number of observations belonging to the other class (the minority class). In loan default prediction, defaults (`1`) are usually much rarer than non-defaults (`0`). This makes the non-default class the majority, and the default class the minority.\n",
        "\n",
        "**Process of Analysis:**\n",
        "\n",
        "1.  **Count Occurrences:** The most straightforward way to analyze the target variable's distribution is to count the occurrences of each class. This can be done using `value_counts()` on the target column in a pandas DataFrame.\n",
        "2.  **Calculate Percentages:** Convert these counts into percentages to clearly see the proportion of each class. For example, if 95% are non-defaults and 5% are defaults, it indicates a significant imbalance.\n",
        "3.  **Visualize Distribution:** Use bar plots (e.g., `seaborn.countplot`, `matplotlib.pyplot.bar`) to visually represent the distribution of the target variable. A bar plot will immediately highlight if one class has a much smaller bar than the other.\n",
        "\n",
        "**Why Class Imbalance is Critical for Loan Default Prediction:**\n",
        "\n",
        "*   **Model Bias:** If a model is trained on an imbalanced dataset, it tends to become biased towards the majority class. It might achieve high overall accuracy by simply predicting the majority class for most instances, but it will perform poorly on the minority class (i.e., identifying actual defaults).\n",
        "*   **Poor Generalization:** Models that ignore class imbalance often fail to generalize well to new, unseen data, especially for the rare but important default cases.\n",
        "*   **Misleading Evaluation Metrics:** Standard accuracy is a misleading metric for imbalanced datasets. A model predicting no defaults might still show 95% accuracy if only 5% of cases are defaults. This is why specialized metrics are needed.\n",
        "\n",
        "**Impact on Modeling Strategy:**\n",
        "\n",
        "Recognizing class imbalance early on is crucial because it dictates subsequent steps in the data science pipeline:\n",
        "\n",
        "*   **Sampling Techniques:** It informs the need for techniques like oversampling (e.g., SMOTE) or undersampling to balance the classes in the training data.\n",
        "*   **Algorithmic Approaches:** It might lead to the selection of algorithms inherently robust to imbalance (like tree-based methods) or those that can incorporate class weights.\n",
        "*   **Evaluation Metrics:** It necessitates using appropriate evaluation metrics beyond simple accuracy, such as precision, recall, F1-score, Area Under the Receiver Operating Characteristic Curve (AUC-ROC), and Area Under the Precision-Recall Curve (AUC-PR), which specifically account for the performance on the minority class.\n",
        "\n",
        "By carefully analyzing the target variable's distribution, a FinTech company can confirm the presence and severity of class imbalance, allowing them to proactively implement strategies to build a robust model capable of accurately identifying loan defaulters, which is the core business objective.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfe33267"
      },
      "source": [
        "### 3. Analyzing Target Variable Distribution and Class Imbalance\n",
        "\n",
        "For a loan default prediction problem, the target variable is typically binary: `0` for non-default and `1` for default. A critical aspect of analyzing this target variable is to understand its distribution, particularly to identify **class imbalance**.\n",
        "\n",
        "**What is Class Imbalance?**\n",
        "\n",
        "Class imbalance occurs when the number of observations belonging to one class (the majority class) significantly outweighs the number of observations belonging to the other class (the minority class). In loan default prediction, defaults (`1`) are usually much rarer than non-defaults (`0`). This makes the non-default class the majority, and the default class the minority.\n",
        "\n",
        "**Process of Analysis:**\n",
        "\n",
        "1.  **Count Occurrences:** The most straightforward way to analyze the target variable's distribution is to count the occurrences of each class. This can be done using `value_counts()` on the target column in a pandas DataFrame.\n",
        "2.  **Calculate Percentages:** Convert these counts into percentages to clearly see the proportion of each class. For example, if 95% are non-defaults and 5% are defaults, it indicates a significant imbalance.\n",
        "3.  **Visualize Distribution:** Use bar plots (e.g., `seaborn.countplot`, `matplotlib.pyplot.bar`) to visually represent the distribution of the target variable. A bar plot will immediately highlight if one class has a much smaller bar than the other.\n",
        "\n",
        "**Why Class Imbalance is Critical for Loan Default Prediction:**\n",
        "\n",
        "*   **Model Bias:** If a model is trained on an imbalanced dataset, it tends to become biased towards the majority class. It might achieve high overall accuracy by simply predicting the majority class for most instances, but it will perform poorly on the minority class (i.e., identifying actual defaults).\n",
        "*   **Poor Generalization:** Models that ignore class imbalance often fail to generalize well to new, unseen data, especially for the rare but important default cases.\n",
        "*   **Misleading Evaluation Metrics:** Standard accuracy is a misleading metric for imbalanced datasets. A model predicting no defaults might still show 95% accuracy if only 5% of cases are defaults. This is why specialized metrics are needed.\n",
        "\n",
        "**Impact on Modeling Strategy:**\n",
        "\n",
        "Recognizing class imbalance early on is crucial because it dictates subsequent steps in the data science pipeline:\n",
        "\n",
        "*   **Sampling Techniques:** It informs the need for techniques like oversampling (e.g., SMOTE) or undersampling to balance the classes in the training data.\n",
        "*   **Algorithmic Approaches:** It might lead to the selection of algorithms inherently robust to imbalance (like tree-based methods) or those that can incorporate class weights.\n",
        "*   **Evaluation Metrics:** It necessitates using appropriate evaluation metrics beyond simple accuracy, such as precision, recall, F1-score, Area Under the Receiver Operating Characteristic Curve (AUC-ROC), and Area Under the Precision-Recall Curve (AUC-PR), which specifically account for the performance on the minority class.\n",
        "\n",
        "By carefully analyzing the target variable's distribution, a FinTech company can confirm the presence and severity of class imbalance, allowing them to proactively implement strategies to build a robust model capable of accurately identifying loan defaulters, which is the core business objective.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9912d8fa"
      },
      "source": [
        "## Data Preprocessing - Handling Missing Values\n",
        "\n",
        "### Subtask:\n",
        "Address missing values in the dataset for both numerical and categorical features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eff770f"
      },
      "source": [
        "```markdown\n",
        "## Data Preprocessing - Handling Missing Values\n",
        "\n",
        "To address missing values in the dataset for both numerical and categorical features, we will follow a structured approach.\n",
        "\n",
        "### 1. Identify the number and percentage of missing values for each feature\n",
        "\n",
        "First, we need to get a clear picture of the extent of missing data in our dataset. This involves calculating the count and proportion of missing values for every column.\n",
        "\n",
        "```python\n",
        "# Assuming 'df' is our dataset\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
        "\n",
        "missing_info = pd.DataFrame({\n",
        "    'Missing Values': missing_values,\n",
        "    'Percentage': missing_percentage\n",
        "})\n",
        "missing_info = missing_info[missing_info['Missing Values'] > 0].sort_values(by='Percentage', ascending=False)\n",
        "\n",
        "print(\"Missing values information:\")\n",
        "print(missing_info)\n",
        "```\n",
        "\n",
        "### 2. Strategy for Numerical Features\n",
        "\n",
        "For numerical features, the imputation strategy will depend on the percentage of missing values and the distribution of the data. Given the potential for skewed distributions in financial data (e.g., income, transaction amounts), the **median** is generally a robust choice for imputation as it is less sensitive to outliers than the mean.\n",
        "\n",
        "*   **Low to Moderate Missingness (e.g., < 30-40%):** Impute with the median.\n",
        "*   **High Missingness (e.g., > 70%):** Consider dropping the column if its predictive power is likely to be very low due to vast missingness, or explore more sophisticated imputation methods like K-Nearest Neighbors (KNN) imputation if the feature is deemed critical and there are strong correlations with other features.\n",
        "\n",
        "```python\n",
        "# Example for numerical imputation\n",
        "# Identify numerical columns with missing values\n",
        "numerical_cols_with_missing = df.select_dtypes(include=['number']).columns[df.select_dtypes(include=['number']).isnull().any()].tolist()\n",
        "\n",
        "for col in numerical_cols_with_missing:\n",
        "    if missing_percentage.loc[col] < 70: # Threshold for dropping\n",
        "        median_val = df[col].median()\n",
        "        df[col].fillna(median_val, inplace=True)\n",
        "        print(f\"Imputed numerical feature '{col}' with median: {median_val:.2f}\")\n",
        "    else:\n",
        "        # Here, we would implement dropping logic or more advanced imputation\n",
        "        print(f\"Numerical feature '{col}' has >70% missing values. Considering dropping or advanced imputation.\")\n",
        "```\n",
        "\n",
        "### 3. Strategy for Categorical Features\n",
        "\n",
        "For categorical features, it's often beneficial to treat 'missingness' as a separate category, as the fact that a value is missing might itself carry predictive information. This is particularly useful for tree-based models like boosting algorithms.\n",
        "\n",
        "*   **Impute with a 'Missing' category:** This is generally the preferred approach for categorical features, especially if the missingness is not extremely rare, as it preserves information about the absence of a value.\n",
        "*   **Impute with the mode:** If missingness is very rare (e.g., < 5%), imputing with the mode might be acceptable to avoid creating too many sparse categories.\n",
        "*   **High Missingness (e.g., > 70%):** Similar to numerical features, consider dropping the column.\n",
        "\n",
        "```python\n",
        "# Example for categorical imputation\n",
        "# Identify categorical columns with missing values\n",
        "categorical_cols_with_missing = df.select_dtypes(include=['object', 'category']).columns[df.select_dtypes(include=['object', 'category']).isnull().any()].tolist()\n",
        "\n",
        "for col in categorical_cols_with_missing:\n",
        "    if missing_percentage.loc[col] < 70:\n",
        "        df[col].fillna('Missing', inplace=True)\n",
        "        print(f\"Imputed categorical feature '{col}' with 'Missing' category.\")\n",
        "    else:\n",
        "        # Here, we would implement dropping logic\n",
        "        print(f\"Categorical feature '{col}' has >70% missing values. Considering dropping.\")\n",
        "```\n",
        "\n",
        "### 4. Document the Chosen Imputation Strategy and Rationale\n",
        "\n",
        "Maintaining a clear record of imputation decisions is crucial for transparency and reproducibility. Here's how we would document it:\n",
        "\n",
        "*   **Feature A (Numerical):** Imputed with median. Rationale: Data distribution is skewed, and the median is robust to outliers. Missing percentage was 15%, which is manageable.\n",
        "*   **Feature B (Categorical):** Imputed with 'Missing' category. Rationale: The absence of a value might be informative, and tree-based models can handle this new category effectively. Missing percentage was 25%.\n",
        "*   **Feature C (Numerical):** Column dropped. Rationale: Over 80% missing values; deemed unlikely to contribute significantly to the model and could introduce noise.\n",
        "*   **Feature D (Categorical):** Imputed with mode. Rationale: Only 2% missing, creating a 'Missing' category would unnecessarily increase feature space for such rare occurrences.\n",
        "\n",
        "This documentation would typically be maintained in a project README, a data dictionary, or inline comments within the data preprocessing script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb3dc05f"
      },
      "source": [
        "## Data Preprocessing - Handling Categorical Features\n",
        "\n",
        "### Subtask:\n",
        "Process categorical features, especially considering their cardinality and the choice of boosting algorithm (CatBoost, XGBoost, or AdaBoost).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c749eaf8"
      },
      "source": [
        "### Introduction to Categorical Feature Handling\n",
        "\n",
        "The existing datasets loaded in the kernel (`Breast Cancer` and `California Housing`) are primarily numerical and do not present a diverse set of categorical features (e.g., 'object' or 'category' dtypes, or numerical features representing categories like zip codes). To properly demonstrate the various strategies for handling categorical features as outlined in the instructions (especially for low-cardinality nominal, high-cardinality nominal, and ordinal features, and the distinction between CatBoost vs. XGBoost/AdaBoost approaches), we will generate a synthetic dataset. This synthetic dataset will simulate a loan default prediction scenario with appropriate feature types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4571c167"
      },
      "source": [
        "**Reasoning**:\n",
        "To properly demonstrate categorical feature handling, I will generate a synthetic dataset that includes numerical, low-cardinality nominal, high-cardinality nominal, and ordinal categorical features, along with a binary target variable for loan default prediction. This will allow for the application of various encoding strategies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fad5283c",
        "outputId": "fff20686-9cbe-40a4-bce3-20cdc3994f99"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Number of samples\n",
        "n_samples = 1000\n",
        "\n",
        "# 1. Numerical Features\n",
        "income = np.random.normal(loc=60000, scale=20000, size=n_samples) # Annual Income\n",
        "age = np.random.randint(22, 65, size=n_samples) # Age\n",
        "loan_amount = np.random.normal(loc=15000, scale=7000, size=n_samples) # Loan Amount\n",
        "credit_score = np.random.randint(300, 850, size=n_samples) # FICO score\n",
        "\n",
        "# 2. Low-Cardinality Nominal Categorical Features\n",
        "gender = np.random.choice(['Male', 'Female'], size=n_samples, p=[0.5, 0.5])\n",
        "marital_status = np.random.choice(['Single', 'Married', 'Divorced'], size=n_samples, p=[0.4, 0.5, 0.1])\n",
        "\n",
        "# 3. High-Cardinality Nominal Categorical Features\n",
        "cities = [f'City_{i}' for i in range(1, 51)] # 50 unique cities\n",
        "city = np.random.choice(cities, size=n_samples, p=np.random.dirichlet(np.ones(len(cities)), size=1)[0])\n",
        "occupation_types = [f'Occupation_{i}' for i in range(1, 31)] # 30 unique occupations\n",
        "occupation = np.random.choice(occupation_types, size=n_samples, p=np.random.dirichlet(np.ones(len(occupation_types)), size=1)[0])\n",
        "\n",
        "# 4. Ordinal Categorical Features\n",
        "education_levels = ['High School', 'Associate', 'Bachelor', 'Master', 'PhD']\n",
        "education = np.random.choice(education_levels, size=n_samples, p=[0.2, 0.2, 0.3, 0.2, 0.1])\n",
        "\n",
        "# 5. Target Variable: Loan Default (binary: 0=No Default, 1=Default)\n",
        "# Simulate some correlation with features\n",
        "default_prob = (0.00001 * (100000 - income) +\n",
        "                0.005 * (60 - age) +\n",
        "                0.00005 * loan_amount -\n",
        "                0.002 * credit_score +\n",
        "                (np.random.rand(n_samples) * 50))\n",
        "\n",
        "# Adjust default probability based on categorical features for demonstration\n",
        "default_prob[gender == 'Female'] -= 10\n",
        "default_prob[marital_status == 'Single'] += 5\n",
        "default_prob[education == 'High School'] += 15\n",
        "default_prob[education == 'PhD'] -= 10\n",
        "default_prob[np.isin(city, ['City_5', 'City_10'])] += 10 # Some cities have higher default rates\n",
        "default_prob[np.isin(occupation, ['Occupation_1', 'Occupation_2'])] += 10 # Some occupations have higher default rates\n",
        "\n",
        "default_prob = np.clip(default_prob / 100, 0.05, 0.5) # Scale to a reasonable probability range\n",
        "loan_default = (np.random.rand(n_samples) < default_prob).astype(int)\n",
        "\n",
        "# Create DataFrame\n",
        "df_loan = pd.DataFrame({\n",
        "    'Income': income,\n",
        "    'Age': age,\n",
        "    'LoanAmount': loan_amount,\n",
        "    'CreditScore': credit_score,\n",
        "    'Gender': gender,\n",
        "    'MaritalStatus': marital_status,\n",
        "    'City': city,\n",
        "    'Occupation': occupation,\n",
        "    'Education': education,\n",
        "    'LoanDefault': loan_default\n",
        "})\n",
        "\n",
        "# Introduce some missing values for demonstration\n",
        "for col in ['Income', 'LoanAmount', 'Gender', 'Education', 'Occupation']:\n",
        "    df_loan.loc[df_loan.sample(frac=0.05, random_state=42).index, col] = np.nan\n",
        "\n",
        "\n",
        "print(\"Synthetic Loan Default Dataset Head:\")\n",
        "print(df_loan.head())\n",
        "print(\"\\nSynthetic Loan Default Dataset Info:\")\n",
        "df_loan.info()\n",
        "\n",
        "# Split features (X) and target (y)\n",
        "X = df_loan.drop('LoanDefault', axis=1)\n",
        "y = df_loan['LoanDefault']\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n",
        "print(f\"Target distribution in training set:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Target distribution in testing set:\\n{y_test.value_counts(normalize=True)}\")\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic Loan Default Dataset Head:\n",
            "         Income  Age    LoanAmount  CreditScore  Gender MaritalStatus  \\\n",
            "0  69934.283060   61  10748.580809          391  Female        Single   \n",
            "1  57234.713977   32  26405.263356          564    Male       Married   \n",
            "2  72953.770762   24  16847.261013          519    Male        Single   \n",
            "3  90460.597128   27  10866.653520          741    Male       Married   \n",
            "4  55316.932506   30  17621.054282          700    Male       Married   \n",
            "\n",
            "      City     Occupation    Education  LoanDefault  \n",
            "0  City_39  Occupation_17       Master            0  \n",
            "1  City_20   Occupation_6     Bachelor            0  \n",
            "2   City_3   Occupation_5    Associate            0  \n",
            "3  City_11  Occupation_15  High School            1  \n",
            "4  City_50  Occupation_14    Associate            0  \n",
            "\n",
            "Synthetic Loan Default Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 10 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   Income         950 non-null    float64\n",
            " 1   Age            1000 non-null   int64  \n",
            " 2   LoanAmount     950 non-null    float64\n",
            " 3   CreditScore    1000 non-null   int64  \n",
            " 4   Gender         950 non-null    object \n",
            " 5   MaritalStatus  1000 non-null   object \n",
            " 6   City           1000 non-null   object \n",
            " 7   Occupation     950 non-null    object \n",
            " 8   Education      950 non-null    object \n",
            " 9   LoanDefault    1000 non-null   int64  \n",
            "dtypes: float64(2), int64(3), object(5)\n",
            "memory usage: 78.3+ KB\n",
            "\n",
            "Training set shape: (700, 9)\n",
            "Testing set shape: (300, 9)\n",
            "Target distribution in training set:\n",
            "LoanDefault\n",
            "0    0.751429\n",
            "1    0.248571\n",
            "Name: proportion, dtype: float64\n",
            "Target distribution in testing set:\n",
            "LoanDefault\n",
            "0    0.753333\n",
            "1    0.246667\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fcd590b"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the synthetic dataset is generated, the next logical step is to address the missing values as outlined in the previous subtask. I will apply the strategies for imputing numerical features with the median and categorical features with a 'Missing' category.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8fb291e",
        "outputId": "8591f3b4-7cf4-4267-b4d1-24ae6c36af88"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Separate features into numerical and categorical for X_train and X_test\n",
        "numerical_cols = X_train.select_dtypes(include=np.number).columns\n",
        "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Impute missing numerical values with the median (fitting on X_train only)\n",
        "for col in numerical_cols:\n",
        "    if X_train[col].isnull().any():\n",
        "        median_val = X_train[col].median()\n",
        "        X_train[col].fillna(median_val, inplace=True)\n",
        "        X_test[col].fillna(median_val, inplace=True) # Impute test set with train median\n",
        "        print(f\"Imputed numerical feature '{col}' with median: {median_val:.2f}\")\n",
        "\n",
        "# Impute missing categorical values with 'Missing' (fitting on X_train only)\n",
        "for col in categorical_cols:\n",
        "    if X_train[col].isnull().any():\n",
        "        X_train[col].fillna('Missing', inplace=True)\n",
        "        X_test[col].fillna('Missing', inplace=True) # Impute test set with 'Missing'\n",
        "        print(f\"Imputed categorical feature '{col}' with 'Missing' category.\")\n",
        "\n",
        "print(\"\\nMissing values after imputation in X_train:\")\n",
        "print(X_train.isnull().sum())\n",
        "print(\"\\nMissing values after imputation in X_test:\")\n",
        "print(X_test.isnull().sum())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imputed numerical feature 'Income' with median: 60552.04\n",
            "Imputed numerical feature 'LoanAmount' with median: 15790.95\n",
            "Imputed categorical feature 'Gender' with 'Missing' category.\n",
            "Imputed categorical feature 'Occupation' with 'Missing' category.\n",
            "Imputed categorical feature 'Education' with 'Missing' category.\n",
            "\n",
            "Missing values after imputation in X_train:\n",
            "Income           0\n",
            "Age              0\n",
            "LoanAmount       0\n",
            "CreditScore      0\n",
            "Gender           0\n",
            "MaritalStatus    0\n",
            "City             0\n",
            "Occupation       0\n",
            "Education        0\n",
            "dtype: int64\n",
            "\n",
            "Missing values after imputation in X_test:\n",
            "Income           0\n",
            "Age              0\n",
            "LoanAmount       0\n",
            "CreditScore      0\n",
            "Gender           0\n",
            "MaritalStatus    0\n",
            "City             0\n",
            "Occupation       0\n",
            "Education        0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1006883345.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train[col].fillna(median_val, inplace=True)\n",
            "/tmp/ipython-input-1006883345.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test[col].fillna(median_val, inplace=True) # Impute test set with train median\n",
            "/tmp/ipython-input-1006883345.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train[col].fillna('Missing', inplace=True)\n",
            "/tmp/ipython-input-1006883345.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test[col].fillna('Missing', inplace=True) # Impute test set with 'Missing'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d869b830"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block generated `FutureWarning` messages regarding the use of `inplace=True` with chained assignment. To address this and adhere to best practices, I will modify the imputation logic to assign the result of `fillna` back to the DataFrame column directly, which is the recommended approach for future pandas versions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e7a4e37",
        "outputId": "393ff651-a4de-40bd-9c53-af8db4f0fff4"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Separate features into numerical and categorical for X_train and X_test\n",
        "numerical_cols = X_train.select_dtypes(include=np.number).columns\n",
        "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Impute missing numerical values with the median (fitting on X_train only)\n",
        "for col in numerical_cols:\n",
        "    if X_train[col].isnull().any():\n",
        "        median_val = X_train[col].median()\n",
        "        X_train.loc[:, col] = X_train[col].fillna(median_val)\n",
        "        X_test.loc[:, col] = X_test[col].fillna(median_val) # Impute test set with train median\n",
        "        print(f\"Imputed numerical feature '{col}' with median: {median_val:.2f}\")\n",
        "\n",
        "# Impute missing categorical values with 'Missing' (fitting on X_train only)\n",
        "for col in categorical_cols:\n",
        "    if X_train[col].isnull().any():\n",
        "        X_train.loc[:, col] = X_train[col].fillna('Missing')\n",
        "        X_test.loc[:, col] = X_test[col].fillna('Missing') # Impute test set with 'Missing'\n",
        "        print(f\"Imputed categorical feature '{col}' with 'Missing' category.\")\n",
        "\n",
        "print(\"\\nMissing values after imputation in X_train:\")\n",
        "print(X_train.isnull().sum())\n",
        "print(\"\\nMissing values after imputation in X_test:\")\n",
        "print(X_test.isnull().sum())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values after imputation in X_train:\n",
            "Income           0\n",
            "Age              0\n",
            "LoanAmount       0\n",
            "CreditScore      0\n",
            "Gender           0\n",
            "MaritalStatus    0\n",
            "City             0\n",
            "Occupation       0\n",
            "Education        0\n",
            "dtype: int64\n",
            "\n",
            "Missing values after imputation in X_test:\n",
            "Income           0\n",
            "Age              0\n",
            "LoanAmount       0\n",
            "CreditScore      0\n",
            "Gender           0\n",
            "MaritalStatus    0\n",
            "City             0\n",
            "Occupation       0\n",
            "Education        0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "953bfe6a"
      },
      "source": [
        "### Categorical Feature Identification and Strategy Selection\n",
        "\n",
        "Based on the synthetic dataset generated, we have the following categorical features:\n",
        "\n",
        "*   **Low-Cardinality Nominal:** `Gender`, `MaritalStatus`\n",
        "    *   These features have a small number of distinct categories and no inherent order. One-Hot Encoding is a suitable strategy for algorithms that cannot handle categorical data directly (like XGBoost, AdaBoost).\n",
        "*   **High-Cardinality Nominal:** `City`, `Occupation`\n",
        "    *   These features have a larger number of distinct categories (50 cities, 30 occupations) and no inherent order. One-Hot Encoding would lead to a very high-dimensional feature space, which can be computationally expensive and potentially lead to the curse of dimensionality. Target Encoding is often preferred here for non-CatBoost models, but must be implemented carefully to avoid target leakage.\n",
        "*   **Ordinal:** `Education`\n",
        "    *   This feature has an inherent order (`High School < Associate < Bachelor < Master < PhD`). Ordinal Encoding is appropriate to preserve this order for algorithms that can utilize it.\n",
        "\n",
        "### Strategy for CatBoost\n",
        "\n",
        "CatBoost is specifically designed to handle categorical features natively. It uses a sophisticated technique called **Ordered Target Encoding** (or more generally, **Ordered Boosting**) that converts categorical features into numerical ones during training without explicit preprocessing. This method effectively prevents target leakage, handles high cardinality well, and avoids increasing dimensionality. Therefore, for CatBoost, the strategy is simply to inform the model which columns are categorical using the `cat_features` parameter.\n",
        "\n",
        "### Strategy for XGBoost and AdaBoost\n",
        "\n",
        "XGBoost and AdaBoost require categorical features to be converted into numerical representations. Our strategy will be:\n",
        "\n",
        "1.  **One-Hot Encoding** for `Gender` and `MaritalStatus`.\n",
        "    *   This creates binary columns for each category, preventing the model from inferring spurious ordinal relationships.\n",
        "2.  **Ordinal Encoding** for `Education`.\n",
        "    *   We will map the education levels to numerical values based on their natural order.\n",
        "3.  **Target Encoding** for `City` and `Occupation`.\n",
        "    *   This involves replacing each category with the mean of the target variable for that category. To prevent data leakage, we will fit the target encoder on the training data and apply it to both training and test sets. We might also use cross-validation based target encoding during training.\n",
        "\n",
        "Let's implement these encoding strategies for XGBoost/AdaBoost, and prepare the data for CatBoost by just identifying the categorical columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d0cd2fa"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the categorical features have been identified and strategies outlined, I will implement the encoding for XGBoost/AdaBoost, applying One-Hot Encoding for low-cardinality nominal features, Ordinal Encoding for ordinal features, and a basic Target Encoding for high-cardinality nominal features. This will create a processed dataset ready for these models, while simultaneously defining the `cat_features` list for CatBoost.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "d598b1a3",
        "outputId": "ea01cd6a-7624-4973-e978-7c68249eaa66"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "from category_encoders import TargetEncoder # Using category_encoders for target encoding\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1. Identify Categorical Features --- #\n",
        "\n",
        "# Columns for CatBoost (CatBoost handles these natively by receiving their names/indices)\n",
        "cat_features_for_catboost = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Columns for XGBoost/AdaBoost (need explicit encoding)\n",
        "low_cardinality_nominal_cols = ['Gender', 'MaritalStatus']\n",
        "high_cardinality_nominal_cols = ['City', 'Occupation']\n",
        "ordinal_cols = ['Education']\n",
        "\n",
        "# Define the order for the ordinal 'Education' feature\n",
        "education_order = ['Missing', 'High School', 'Associate', 'Bachelor', 'Master', 'PhD']\n",
        "\n",
        "# --- 2. Initialize Encoders --- #\n",
        "\n",
        "# One-Hot Encoder\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "# Ordinal Encoder\n",
        "ord_enc = OrdinalEncoder(categories=[education_order], handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "\n",
        "# Target Encoder (instantiate for each high-cardinality feature if preferred, or one for all)\n",
        "# IMPORTANT: For proper target encoding without leakage, typically use KFoldTargetEncoder or fit on CV splits.\n",
        "# For this demonstration, we'll fit on train and transform both train/test.\n",
        "# In a real scenario, use more robust techniques to avoid information leakage.\n",
        "# I'll create a dictionary to store one encoder per high-cardinality feature.\n",
        "target_encoders = {}\n",
        "for col in high_cardinality_nominal_cols:\n",
        "    target_encoders[col] = TargetEncoder(cols=[col])\n",
        "\n",
        "# --- 3. Apply Encodings to X_train and X_test --- #\n",
        "\n",
        "# Create copies to avoid modifying original X_train, X_test during encoding\n",
        "X_train_encoded = X_train.copy()\n",
        "X_test_encoded = X_test.copy()\n",
        "\n",
        "# One-Hot Encoding\n",
        "ohe.fit(X_train[low_cardinality_nominal_cols]) # Fit on training data\n",
        "ohe_train = ohe.transform(X_train[low_cardinality_nominal_cols])\n",
        "ohe_test = ohe.transform(X_test[low_cardinality_nominal_cols])\n",
        "\n",
        "ohe_feature_names = ohe.get_feature_names_out(low_cardinality_nominal_cols)\n",
        "X_train_encoded = pd.concat([X_train_encoded.drop(columns=low_cardinality_nominal_cols),\n",
        "                             pd.DataFrame(ohe_train, columns=ohe_feature_names, index=X_train.index)], axis=1)\n",
        "X_test_encoded = pd.concat([X_test_encoded.drop(columns=low_cardinality_nominal_cols),\n",
        "                            pd.DataFrame(ohe_test, columns=ohe_feature_names, index=X_test.index)], axis=1)\n",
        "\n",
        "# Ordinal Encoding\n",
        "ord_enc.fit(X_train[ordinal_cols]) # Fit on training data\n",
        "X_train_encoded[ordinal_cols] = ord_enc.transform(X_train_encoded[ordinal_cols])\n",
        "X_test_encoded[ordinal_cols] = ord_enc.transform(X_test_encoded[ordinal_cols])\n",
        "\n",
        "# Target Encoding\n",
        "for col in high_cardinality_nominal_cols:\n",
        "    # Fit on training data (X_train and y_train) and transform both train and test\n",
        "    X_train_encoded[col] = target_encoders[col].fit_transform(X_train[[col]], y_train)\n",
        "    X_test_encoded[col] = target_encoders[col].transform(X_test[[col]])\n",
        "\n",
        "print(\"--- Encoded Data for XGBoost/AdaBoost ---\")\n",
        "print(\"X_train_encoded head:\")\n",
        "print(X_train_encoded.head())\n",
        "print(f\"\\nX_train_encoded shape: {X_train_encoded.shape}\")\n",
        "print(f\"X_test_encoded shape: {X_test_encoded.shape}\")\n",
        "\n",
        "print(\"\\n--- Categorical Features for CatBoost (names) ---\")\n",
        "print(cat_features_for_catboost)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'category_encoders'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2371834037.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOrdinalEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcategory_encoders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTargetEncoder\u001b[0m \u001b[0;31m# Using category_encoders for target encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# --- 1. Identify Categorical Features --- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb61ffd2"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the `category_encoders` library was not installed. I need to install it before attempting to import and use `TargetEncoder`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaa0792f",
        "outputId": "a3eb6395-cc38-4c8d-f134-ce3b67f5d3b6"
      },
      "source": [
        "!pip install category_encoders"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.9.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.16.3)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (0.14.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Downloading category_encoders-2.9.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.9/85.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68490e2e"
      },
      "source": [
        "**Reasoning**:\n",
        "The `category_encoders` library has been successfully installed. Now I can re-execute the previous code block that performs the categorical encoding, which will apply One-Hot Encoding, Ordinal Encoding, and Target Encoding to the relevant categorical features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c2fbe05",
        "outputId": "fb51d05f-b983-48b8-db6b-82c58f19c7c6"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "from category_encoders import TargetEncoder # Using category_encoders for target encoding\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1. Identify Categorical Features --- #\n",
        "\n",
        "# Columns for CatBoost (CatBoost handles these natively by receiving their names/indices)\n",
        "cat_features_for_catboost = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Columns for XGBoost/AdaBoost (need explicit encoding)\n",
        "low_cardinality_nominal_cols = ['Gender', 'MaritalStatus']\n",
        "high_cardinality_nominal_cols = ['City', 'Occupation']\n",
        "ordinal_cols = ['Education']\n",
        "\n",
        "# Define the order for the ordinal 'Education' feature\n",
        "education_order = ['Missing', 'High School', 'Associate', 'Bachelor', 'Master', 'PhD']\n",
        "\n",
        "# --- 2. Initialize Encoders --- #\n",
        "\n",
        "# One-Hot Encoder\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "# Ordinal Encoder\n",
        "ord_enc = OrdinalEncoder(categories=[education_order], handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "\n",
        "# Target Encoder (instantiate for each high-cardinality feature if preferred, or one for all)\n",
        "# IMPORTANT: For proper target encoding without leakage, typically use KFoldTargetEncoder or fit on CV splits.\n",
        "# For this demonstration, we'll fit on train and transform both train/test.\n",
        "# In a real scenario, use more robust techniques to avoid information leakage.\n",
        "# I'll create a dictionary to store one encoder per high-cardinality feature.\n",
        "target_encoders = {}\n",
        "for col in high_cardinality_nominal_cols:\n",
        "    target_encoders[col] = TargetEncoder(cols=[col])\n",
        "\n",
        "# --- 3. Apply Encodings to X_train and X_test --- #\n",
        "\n",
        "# Create copies to avoid modifying original X_train, X_test during encoding\n",
        "X_train_encoded = X_train.copy()\n",
        "X_test_encoded = X_test.copy()\n",
        "\n",
        "# One-Hot Encoding\n",
        "ohe.fit(X_train[low_cardinality_nominal_cols]) # Fit on training data\n",
        "ohe_train = ohe.transform(X_train[low_cardinality_nominal_cols])\n",
        "ohe_test = ohe.transform(X_test[low_cardinality_nominal_cols])\n",
        "\n",
        "ohe_feature_names = ohe.get_feature_names_out(low_cardinality_nominal_cols)\n",
        "X_train_encoded = pd.concat([X_train_encoded.drop(columns=low_cardinality_nominal_cols),\n",
        "                             pd.DataFrame(ohe_train, columns=ohe_feature_names, index=X_train.index)], axis=1)\n",
        "X_test_encoded = pd.concat([X_test_encoded.drop(columns=low_cardinality_nominal_cols),\n",
        "                            pd.DataFrame(ohe_test, columns=ohe_feature_names, index=X_test.index)], axis=1)\n",
        "\n",
        "# Ordinal Encoding\n",
        "ord_enc.fit(X_train[ordinal_cols]) # Fit on training data\n",
        "X_train_encoded[ordinal_cols] = ord_enc.transform(X_train_encoded[ordinal_cols])\n",
        "X_test_encoded[ordinal_cols] = ord_enc.transform(X_test_encoded[ordinal_cols])\n",
        "\n",
        "# Target Encoding\n",
        "for col in high_cardinality_nominal_cols:\n",
        "    # Fit on training data (X_train and y_train) and transform both train and test\n",
        "    X_train_encoded[col] = target_encoders[col].fit_transform(X_train[[col]], y_train)\n",
        "    X_test_encoded[col] = target_encoders[col].transform(X_test[[col]])\n",
        "\n",
        "print(\"--- Encoded Data for XGBoost/AdaBoost ---\")\n",
        "print(\"X_train_encoded head:\")\n",
        "print(X_train_encoded.head())\n",
        "print(f\"\\nX_train_encoded shape: {X_train_encoded.shape}\")\n",
        "print(f\"X_test_encoded shape: {X_test_encoded.shape}\")\n",
        "\n",
        "print(\"\\n--- Categorical Features for CatBoost (names) ---\")\n",
        "print(cat_features_for_catboost)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Encoded Data for XGBoost/AdaBoost ---\n",
            "X_train_encoded head:\n",
            "           Income  Age    LoanAmount  CreditScore      City  Occupation  \\\n",
            "829  54364.307823   34  12396.887874          447  0.291367    0.230499   \n",
            "507  42720.184606   63   9236.485568          307  0.274850    0.179895   \n",
            "67   80070.657958   53  10604.584342          326  0.166171    0.255713   \n",
            "553  65088.416866   25  14890.435569          364  0.243072    0.210176   \n",
            "698  78480.540384   32  19991.428266          610  0.290807    0.217394   \n",
            "\n",
            "     Education  Gender_Female  Gender_Male  Gender_Missing  \\\n",
            "829        3.0            0.0          1.0             0.0   \n",
            "507        3.0            0.0          1.0             0.0   \n",
            "67         2.0            0.0          1.0             0.0   \n",
            "553        3.0            1.0          0.0             0.0   \n",
            "698        4.0            1.0          0.0             0.0   \n",
            "\n",
            "     MaritalStatus_Divorced  MaritalStatus_Married  MaritalStatus_Single  \n",
            "829                     0.0                    0.0                   1.0  \n",
            "507                     0.0                    0.0                   1.0  \n",
            "67                      0.0                    1.0                   0.0  \n",
            "553                     0.0                    0.0                   1.0  \n",
            "698                     0.0                    1.0                   0.0  \n",
            "\n",
            "X_train_encoded shape: (700, 13)\n",
            "X_test_encoded shape: (300, 13)\n",
            "\n",
            "--- Categorical Features for CatBoost (names) ---\n",
            "['Gender', 'MaritalStatus', 'City', 'Occupation', 'Education']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "085cb0da"
      },
      "source": [
        "## Addressing Data Imbalance\n",
        "\n",
        "### Subtask:\n",
        "Implement strategies to address the identified class imbalance in the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0df9b2c"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I need to calculate the ratio of the majority class to the minority class in the training set (`y_train`) to understand the extent of the class imbalance, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52586307",
        "outputId": "3d4c2dde-d790-4121-841d-cbca44940d5d"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Get class distribution in the training target\n",
        "class_counts = Counter(y_train)\n",
        "\n",
        "# Identify majority and minority classes\n",
        "majority_class_label = max(class_counts, key=class_counts.get)\n",
        "minority_class_label = min(class_counts, key=class_counts.get)\n",
        "\n",
        "count_majority = class_counts[majority_class_label]\n",
        "count_minority = class_counts[minority_class_label]\n",
        "\n",
        "# Calculate the imbalance ratio\n",
        "imbalance_ratio = count_majority / count_minority\n",
        "\n",
        "print(f\"Class distribution in training data: {class_counts}\")\n",
        "print(f\"Majority class ({majority_class_label}) count: {count_majority}\")\n",
        "print(f\"Minority class ({minority_class_label}) count: {count_minority}\")\n",
        "print(f\"Imbalance Ratio (Majority / Minority): {imbalance_ratio:.2f}\")\n",
        "\n",
        "# Calculate scale_pos_weight for XGBoost/LightGBM or class_weights for CatBoost\n",
        "scale_pos_weight_value = imbalance_ratio\n",
        "print(f\"Calculated 'scale_pos_weight' (for XGBoost/LightGBM) or base for 'class_weights' (for CatBoost): {scale_pos_weight_value:.2f}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution in training data: Counter({0: 526, 1: 174})\n",
            "Majority class (0) count: 526\n",
            "Minority class (1) count: 174\n",
            "Imbalance Ratio (Majority / Minority): 3.02\n",
            "Calculated 'scale_pos_weight' (for XGBoost/LightGBM) or base for 'class_weights' (for CatBoost): 3.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6ecb40f"
      },
      "source": [
        "### Strategy to Address Class Imbalance: Using Class Weights\n",
        "\n",
        "Given the identified class imbalance (majority class 0, minority class 1, with an imbalance ratio of approximately 3.02:1), we will address this by setting **class weights** directly within the boosting algorithms. This method is preferred over techniques like SMOTE (Synthetic Minority Over-sampling Technique) in this context for several reasons:\n",
        "\n",
        "1.  **Direct Algorithm Support**: Boosting algorithms like XGBoost and CatBoost have built-in parameters (`scale_pos_weight` for XGBoost/LightGBM, `class_weights` for CatBoost) to handle class imbalance. This allows the model to inherently pay more attention to the minority class during training by penalizing misclassifications of the minority class more heavily.\n",
        "2.  **Avoids Synthetic Data Issues**: SMOTE generates synthetic samples, which can sometimes introduce noise or unrealistic data points, especially in high-dimensional spaces or when the minority class is very sparse. Using class weights avoids altering the original data distribution in this way.\n",
        "3.  **Simplicity and Efficiency**: Integrating class weights is often simpler to implement and can be more computationally efficient than generating new synthetic data points, especially for large datasets.\n",
        "4.  **Preserves Original Data**: This approach ensures that the model learns from the actual observed data points without artificial augmentation.\n",
        "\n",
        "#### Implementation Detail:\n",
        "\n",
        "*   For **XGBoost**, the `scale_pos_weight` parameter will be set to the ratio of the count of the negative class (majority) to the count of the positive class (minority). From our previous calculation, this value is approximately `3.02`.\n",
        "*   For **CatBoost**, the `class_weights` parameter can be used, typically specified as a list or dictionary, assigning a higher weight to the minority class. For example, `class_weights=[1, imbalance_ratio]` where `1` is the weight for the majority class and `imbalance_ratio` for the minority class.\n",
        "\n",
        "This strategy will ensure that the models are not biased towards the majority class and are better at identifying the rare but critical loan default events, which is crucial for the FinTech company's objective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f672e4d"
      },
      "source": [
        "## Model Selection (Boosting Technique)\n",
        "\n",
        "### Subtask:\n",
        "Select an appropriate boosting algorithm for the loan default prediction task, specifically choosing between AdaBoost, XGBoost, and CatBoost, and justify the choice.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3855f84"
      },
      "source": [
        "## Model Selection (Boosting Technique)\n",
        "\n",
        "### Subtask:\n",
        "Select an appropriate boosting algorithm for the loan default prediction task, specifically choosing between AdaBoost, XGBoost, and CatBoost, and justify the choice.\n",
        "\n",
        "#### 1. Selection of Boosting Algorithm: CatBoost\n",
        "\n",
        "For the loan default prediction task with an imbalanced dataset, missing values, and both numeric and diverse categorical features (including high-cardinality nominal and ordinal), **CatBoost** is the most appropriate boosting algorithm.\n",
        "\n",
        "#### 2. Justification of Choice\n",
        "\n",
        "CatBoost's design inherently addresses several of the challenges presented by this dataset, making it a superior choice compared to AdaBoost and XGBoost for this specific problem:\n",
        "\n",
        "*   **Native Handling of Categorical Features (Ordered Target Encoding):** This is CatBoost's most significant advantage. Our dataset contains low-cardinality nominal, high-cardinality nominal (`City`, `Occupation`), and ordinal categorical features. Traditional methods like One-Hot Encoding for high-cardinality features would lead to a massive increase in dimensionality and computational burden, while naive Target Encoding risks data leakage and overfitting. CatBoost's **Ordered Target Encoding** (and more generally, Ordered Boosting) handles these features directly and robustly, converting them into numerical representations during training in a way that prevents target leakage and effectively captures their predictive power. This eliminates the need for extensive manual categorical preprocessing, which simplifies the pipeline and improves model generalization.\n",
        "\n",
        "*   **Robustness to Imbalance (Class Weights):** While all three algorithms can incorporate class weights, CatBoost provides native support through `class_weights` or `auto_class_weights` parameters. This allows the model to give more importance to the minority class (defaults) during training, preventing it from being overshadowed by the majority class (non-defaults). This is crucial for accurately identifying the rare but critical default cases.\n",
        "\n",
        "*   **Handling of Missing Values:** CatBoost, like XGBoost, has built-in mechanisms to handle missing values without explicit imputation in some cases. However, our preprocessing step has already imputed missing values strategically. CatBoost's robustness to various data types complements this imputation by effectively integrating the imputed data.\n",
        "\n",
        "*   **Strong Regularization and Overfitting Prevention (Symmetric Trees, Ordered Boosting):** CatBoost employs symmetric trees and an 'ordered boosting' scheme to reduce prediction shift and overfitting. This is vital for a financial application like loan default, where model stability and generalization to unseen data are paramount. The financial domain often features complex, noisy data, and CatBoost's regularization helps build a more reliable model.\n",
        "\n",
        "*   **Ease of Use:** CatBoost often performs well out-of-the-box with minimal hyperparameter tuning, especially with its native categorical feature handling, which streamlines the development process.\n",
        "\n",
        "#### 3. Comparison with Other Candidates\n",
        "\n",
        "*   **Compared to AdaBoost:**\n",
        "    *   **Categorical Features:** AdaBoost requires all categorical features to be pre-encoded manually (e.g., One-Hot, Ordinal, Target Encoding). For high-cardinality features, this adds significant complexity and potential for issues like increased dimensionality or leakage if not handled carefully. CatBoost handles these natively and more effectively.\n",
        "    *   **Performance:** AdaBoost is generally less powerful and less robust than gradient boosting algorithms (XGBoost, CatBoost) on complex, noisy, and high-dimensional datasets. It is also more sensitive to outliers, which can be an issue in financial data.\n",
        "    *   **Imbalance:** While AdaBoost can be adapted for imbalance by adjusting sample weights or using specific loss functions, it is not as inherently robust as CatBoost's dedicated class weighting.\n",
        "\n",
        "*   **Compared to XGBoost:**\n",
        "    *   **Categorical Features:** XGBoost requires categorical features to be numerically encoded prior to training. While it can handle One-Hot and Ordinal Encoded features, it does not have an intrinsic mechanism like CatBoost for high-cardinality features. Using Target Encoding with XGBoost would necessitate careful implementation (e.g., K-Fold Target Encoding) to prevent target leakage, adding complexity to the preprocessing pipeline. CatBoost automates this process internally.\n",
        "    *   **Missing Values:** Both XGBoost and CatBoost handle missing values intelligently by default, which is a strong point for both. However, our pipeline already addressed this via imputation.\n",
        "    *   **Regularization:** Both offer strong regularization capabilities to prevent overfitting. XGBoost is highly optimized and widely used, but CatBoost's specific approach to categorical features gives it an edge here.\n",
        "    *   **Imbalance:** XGBoost effectively handles imbalance using the `scale_pos_weight` parameter, which is comparable to CatBoost's `class_weights`.\n",
        "\n",
        "In conclusion, while XGBoost is a powerful contender, CatBoost's native and sophisticated handling of diverse categorical features, especially high-cardinality ones, makes it the most advantageous choice for this loan default prediction problem, simplifying the preprocessing pipeline and promising better generalization and robustness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed6bad37"
      },
      "source": [
        "## Hyperparameter Tuning Strategy\n",
        "\n",
        "### Subtask:\n",
        "Implement a robust hyperparameter tuning strategy for the chosen CatBoost model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c65203a2"
      },
      "source": [
        "### Hyperparameter Tuning Strategy for CatBoost\n",
        "\n",
        "To implement a robust hyperparameter tuning strategy for the chosen CatBoost model, we will follow these steps:\n",
        "\n",
        "1.  **Define a Search Space for Key Hyperparameters:**\n",
        "    We will focus on a subset of CatBoost's important hyperparameters that significantly influence model performance and generalization capabilities:\n",
        "    *   `iterations` (number of trees): Controls the number of boosting rounds. A higher number can lead to better performance but also increases the risk of overfitting and computational cost.\n",
        "    *   `learning_rate`: Determines the step size shrinkage at each step. Smaller learning rates require more iterations but can lead to more robust models.\n",
        "    *   `depth`: Maximum depth of the individual decision trees. Controls the complexity of the weak learners.\n",
        "    *   `l2_leaf_reg` (L2 regularization coefficient): Regularization term that helps prevent overfitting by penalizing large leaf weights.\n",
        "    *   `subsample`: The fraction of observations to be used for fitting the individual base learners. This helps reduce variance and prevents overfitting (similar to bagging).\n",
        "\n",
        "2.  **Choose a Hyperparameter Optimization Technique:**\n",
        "    Given the need for thorough exploration and the availability of computational resources in a typical FinTech environment, **`GridSearchCV`** from `sklearn.model_selection` will be used for an initial, more exhaustive search over a predefined grid of hyperparameters. While `RandomizedSearchCV` is faster for very large search spaces, `GridSearchCV` provides a systematic way to explore combinations. For more advanced scenarios or larger datasets, advanced Bayesian optimization libraries like Optuna or Hyperopt would be considered for their efficiency in finding optimal hyperparameters.\n",
        "\n",
        "3.  **Integrate Cross-Validation:**\n",
        "    To ensure robust evaluation of hyperparameter combinations and to prevent overfitting to a single train-test split, **K-Fold Cross-Validation** (e.g., `cv=3` or `cv=5`) will be integrated directly within the `GridSearchCV` process. This provides a more reliable estimate of the model's performance on unseen data.\n",
        "\n",
        "4.  **Incorporate Early Stopping:**\n",
        "    CatBoost supports early stopping, which is crucial for preventing overfitting and saving computational resources during tuning. During the training of each CatBoost model within the `GridSearchCV` folds, we will monitor a validation metric (e.g., `eval_metric='F1'` or `'AUC'`) on a separate validation set (or a portion of the training data used for internal validation by CatBoost). Training will stop if the validation metric does not improve for a certain number of consecutive iterations (`early_stopping_rounds`).\n",
        "\n",
        "5.  **Ensure Appropriate Evaluation Metrics for Imbalanced Datasets:**\n",
        "    For a loan default prediction problem with an imbalanced dataset, standard accuracy is misleading. Therefore, the hyperparameter tuning process will evaluate models based on metrics more suitable for class imbalance, specifically:\n",
        "    *   **Area Under the Precision-Recall Curve (PR-AUC)**: Often preferred over ROC-AUC for highly imbalanced datasets, as it focuses on the performance of the minority class (defaults).\n",
        "    *   **F1-score**: The harmonic mean of precision and recall, providing a balanced measure that penalizes models with many false positives and false negatives.\n",
        "\n",
        "    We will set `scoring='f1'` (or `'roc_auc'` which is robust) in `GridSearchCV` to guide the search for the best model, but ultimately PR-AUC will be a key metric for final model selection and business impact assessment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25978a05"
      },
      "source": [
        "**Reasoning**:\n",
        "I will implement the hyperparameter tuning strategy for the CatBoost Classifier using `GridSearchCV`, defining a search space, using `StratifiedKFold` for cross-validation, applying `scale_pos_weight` to address class imbalance, and evaluating with `roc_auc` as a robust metric for imbalanced datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca9f4005",
        "outputId": "0433108b-fac9-45fc-c999-0c4f2a11e2de"
      },
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve, auc\n",
        "import numpy as np\n",
        "\n",
        "# Define the search space for hyperparameters\n",
        "# Keeping the search space relatively small for faster execution in a notebook environment\n",
        "param_grid = {\n",
        "    'iterations': [100, 200], # Number of trees\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'depth': [4, 6],\n",
        "    'l2_leaf_reg': [1, 3], # L2 regularization coefficient\n",
        "    'subsample': [0.7, 0.9] # Fraction of observations to use for fitting each tree\n",
        "}\n",
        "\n",
        "# Calculate scale_pos_weight for class imbalance\n",
        "# 'imbalance_ratio' was calculated in a previous step\n",
        "# Ensure it's a float; for binary classification, it's (count_majority / count_minority)\n",
        "scale_pos_weight_value = imbalance_ratio # From previous calculation\n",
        "\n",
        "# Initialize CatBoost Classifier with fixed parameters and class weights\n",
        "# Use verbose=0 to suppress training output during grid search\n",
        "# Use loss_function='Logloss' for binary classification\n",
        "catboost_base_model = CatBoostClassifier(\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='AUC', # Use AUC as eval_metric for internal early stopping monitoring\n",
        "    random_seed=42,\n",
        "    verbose=0,\n",
        "    early_stopping_rounds=10, # Stop if AUC on validation set doesn't improve for 10 rounds\n",
        "    scale_pos_weight=scale_pos_weight_value,\n",
        "    # For handling categorical features, CatBoost needs to know which columns are categorical\n",
        "    # cat_features parameter will be passed during the fit method of GridSearchCV\n",
        ")\n",
        "\n",
        "# Setup StratifiedKFold for cross-validation to maintain class distribution\n",
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# Scoring with 'roc_auc' is suitable for imbalanced classification tasks\n",
        "# We'll also specifically calculate PR-AUC after finding the best model\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=catboost_base_model,\n",
        "    param_grid=param_grid,\n",
        "    scoring='roc_auc',\n",
        "    cv=skf,\n",
        "    verbose=1, # Print progress during tuning\n",
        "    n_jobs=-1 # Use all available CPU cores\n",
        ")\n",
        "\n",
        "print(\"Starting CatBoost Hyperparameter Tuning with GridSearchCV...\")\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "# Pass 'cat_features' to the fit method to leverage CatBoost's native categorical handling\n",
        "# We use the original X_train (after missing value imputation, but before explicit encoding for other models)\n",
        "\n",
        "grid_search.fit(X_train, y_train, cat_features=cat_features_for_catboost)\n",
        "\n",
        "print(\"CatBoost Hyperparameter Tuning Complete.\")\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(f\"\\nBest parameters found: {grid_search.best_params_}\")\n",
        "\n",
        "# Get the best model\n",
        "best_catboost_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "# Predict probabilities for ROC AUC and PR AUC calculation\n",
        "y_pred_proba = best_catboost_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"Best CatBoost Model ROC AUC on test set: {roc_auc:.4f}\")\n",
        "\n",
        "# Calculate Precision-Recall AUC score\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f\"Best CatBoost Model PR AUC on test set: {pr_auc:.4f}\")\n",
        "\n",
        "# To demonstrate F1-score, we need to choose a threshold. For simplicity, we can use 0.5\n",
        "y_pred_class = (y_pred_proba > 0.5).astype(int)\n",
        "f1 = f1_score(y_test, y_pred_class)\n",
        "print(f\"Best CatBoost Model F1-score (threshold=0.5) on test set: {f1:.4f}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting CatBoost Hyperparameter Tuning with GridSearchCV...\n",
            "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
            "CatBoost Hyperparameter Tuning Complete.\n",
            "\n",
            "Best parameters found: {'depth': 4, 'iterations': 200, 'l2_leaf_reg': 3, 'learning_rate': 0.2, 'subsample': 0.9}\n",
            "Best CatBoost Model ROC AUC on test set: 0.5031\n",
            "Best CatBoost Model PR AUC on test set: 0.2706\n",
            "Best CatBoost Model F1-score (threshold=0.5) on test set: 0.2238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9f57240"
      },
      "source": [
        "## Model Evaluation Metrics\n",
        "\n",
        "### Subtask:\n",
        "Calculate and interpret key evaluation metrics for the best CatBoost model, including Precision, Recall, F1-Score, ROC-AUC, and PR-AUC, emphasizing their significance for an imbalanced loan default prediction problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b17c4f9a"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to calculate the precision and recall scores using the previously obtained `y_test` and `y_pred_class` to complete the metric calculations requested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b663f136",
        "outputId": "0c292f23-ff69-4327-f7b1-185cafb1fb59"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "# Calculate Precision\n",
        "precision = precision_score(y_test, y_pred_class)\n",
        "print(f\"Best CatBoost Model Precision on test set: {precision:.4f}\")\n",
        "\n",
        "# Calculate Recall\n",
        "recall = recall_score(y_test, y_pred_class)\n",
        "print(f\"Best CatBoost Model Recall on test set: {recall:.4f}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best CatBoost Model Precision on test set: 0.2319\n",
            "Best CatBoost Model Recall on test set: 0.2162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f99a1f0b"
      },
      "source": [
        "### Interpretation of Evaluation Metrics for Loan Default Prediction\n",
        "\n",
        "We have calculated the following evaluation metrics for our best CatBoost model on the test set:\n",
        "\n",
        "*   **ROC AUC**: 0.5031\n",
        "*   **PR AUC**: 0.2706\n",
        "*   **F1-score (threshold=0.5)**: 0.2238\n",
        "*   **Precision (threshold=0.5)**: 0.2319\n",
        "*   **Recall (threshold=0.5)**: 0.2162\n",
        "\n",
        "Let's interpret these results in the context of loan default prediction for a FinTech company, considering the dataset's class imbalance (imbalance ratio of approximately 3.02:1, meaning 3 non-defaults for every 1 default).\n",
        "\n",
        "#### 1. ROC AUC (Area Under the Receiver Operating Characteristic Curve)\n",
        "\n",
        "*   **Value**: `0.5031`\n",
        "*   **Interpretation**: The ROC AUC score measures the model's ability to distinguish between the positive class (loan default) and the negative class (no default) across all possible classification thresholds. A score of 0.5 indicates that the model is performing no better than random guessing. A perfect classifier would have an ROC AUC of 1.0. Our model's ROC AUC being barely above 0.5 suggests it has very little discriminative power between defaulters and non-defaulters.\n",
        "*   **Significance for FinTech**: In loan default, we want a model that can reliably rank applicants from lowest to highest risk. A low ROC AUC means the model struggles to consistently place true defaulters higher in its probability predictions than true non-defaulters. This makes it difficult to set effective risk-based pricing or approval strategies.\n",
        "\n",
        "#### 2. PR AUC (Area Under the Precision-Recall Curve)\n",
        "\n",
        "*   **Value**: `0.2706`\n",
        "*   **Interpretation**: PR AUC is particularly informative for highly imbalanced datasets, as it focuses on the performance of the minority class. Precision measures the accuracy of positive predictions (of all predicted defaults, how many were actual defaults), while Recall measures the completeness of positive predictions (of all actual defaults, how many were correctly identified). A higher PR AUC indicates better performance on the minority class. For our dataset, a PR AUC of 0.2706 suggests that the model is not very effective at finding actual defaulters without also identifying a large number of false positives, or it's missing a lot of true defaulters.\n",
        "*   **Significance for FinTech**: For a FinTech company, incorrectly identifying a non-defaulter as a defaulter (false positive) means denying a potentially good customer, losing revenue. Missing an actual defaulter (false negative) leads to financial losses. PR AUC directly assesses this trade-off, and a low score implies significant challenges in both accurately identifying risky customers and retaining profitable ones.\n",
        "\n",
        "#### 3. F1-Score\n",
        "\n",
        "*   **Value (threshold=0.5)**: `0.2238`\n",
        "*   **Interpretation**: The F1-score is the harmonic mean of precision and recall. It provides a single metric that balances both concerns. A low F1-score like 0.2238 (on a synthetic dataset with ~25% defaults, this is quite poor) indicates that the model is struggling to achieve a good balance between precision and recall. It's either making too many false positive predictions, missing too many true positive predictions, or both.\n",
        "*   **Significance for FinTech**: A balanced metric like F1-score is important because both types of errors (false positives and false negatives) have significant business costs. A low F1-score suggests the model is not providing a reliable basis for making critical loan decisions.\n",
        "\n",
        "#### 4. Precision\n",
        "\n",
        "*   **Value (threshold=0.5)**: `0.2319`\n",
        "*   **Interpretation**: This means that when the model predicts an applicant will default, it is correct only about 23.19% of the time. The remaining ~76.81% of those predicted to default are actually non-defaulters (false positives).\n",
        "*   **Significance for FinTech**: High precision is critical for the FinTech's bottom line. Denying a loan based on a model's prediction is a serious action. If precision is low, the company is denying loans to a high number of creditworthy applicants, resulting in lost business and potential damage to reputation. It also means that resources spent on further due diligence for predicted defaulters are largely wasted.\n",
        "\n",
        "#### 5. Recall\n",
        "\n",
        "*   **Value (threshold=0.5)**: `0.2162`\n",
        "*   **Interpretation**: This means that the model is only identifying about 21.62% of all actual defaulters. The remaining ~78.38% of actual defaulters are being missed by the model (false negatives).\n",
        "*   **Significance for FinTech**: High recall is crucial for mitigating financial risk. Missing a large percentage of actual defaulters means the company is extending loans to many high-risk individuals who will eventually default, leading to significant financial losses from unpaid loans. While high precision reduces customer friction, high recall directly prevents substantial monetary losses.\n",
        "\n",
        "#### Why these metrics are important beyond simple accuracy:\n",
        "\n",
        "*   **Accuracy is Misleading**: In imbalanced datasets like loan default, a model can achieve high accuracy by simply predicting the majority class (no default) for almost all cases. For example, if only 5% of loans default, a model predicting *no defaults for anyone* would still achieve 95% accuracy. This model is useless for the business purpose of identifying defaulters. The calculated metrics like F1, Precision, Recall, ROC AUC, and PR AUC specifically account for the model's performance on the minority (important) class and its ability to distinguish classes, thus providing a much more truthful and actionable assessment of model quality.\n",
        "\n",
        "**Overall Conclusion**: The current model's performance, as indicated by these metrics, is very poor. An ROC AUC near 0.5 and very low Precision, Recall, and F1-score suggest that the model, in its current state, is not suitable for deployment in a FinTech company for loan default prediction. Further work is needed, potentially including more sophisticated feature engineering, exploring different model architectures, or more advanced imbalance handling techniques, and a more extensive hyperparameter tuning search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "526f09d3"
      },
      "source": [
        "## Business Benefits and Interpretation\n",
        "\n",
        "### Subtask:\n",
        "Describe how the model benefits the FinTech company, considering its current performance, and emphasize the importance of interpretability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "336e0625"
      },
      "source": [
        "## Business Benefits and Interpretation\n",
        "\n",
        "### Current Model Performance Acknowledgment\n",
        "\n",
        "Based on the calculated evaluation metrics – ROC AUC of 0.5031, PR AUC of 0.2706, F1-score of 0.2238, Precision of 0.2319, and Recall of 0.2162 – it is evident that the **current CatBoost model's performance is not suitable for direct deployment** in a real-world loan default prediction system. An ROC AUC value close to 0.5 indicates that the model is performing no better than random guessing. Similarly, the low F1-score, Precision, and Recall values suggest that the model is not effectively identifying default cases, which is the minority and critical class. This highlights that significant further work is required to improve the model's predictive power.\n",
        "\n",
        "### Potential Business Benefits of a Well-Performing Model\n",
        "\n",
        "Despite the current model's limitations, a **well-performing loan default prediction model** would bring substantial benefits to a FinTech company:\n",
        "\n",
        "1.  **Proactive Risk Management:** Accurately identifying potential defaulters enables the company to take proactive measures, such as offering financial counseling, restructuring loans, or adjusting credit limits, before a default occurs. This moves from reactive loss mitigation to proactive risk management.\n",
        "2.  **Reduced Financial Losses:** By predicting and mitigating defaults, the company can significantly reduce charge-offs and non-performing loans, directly impacting profitability and financial stability.\n",
        "3.  **Optimized Loan Approval Processes:** A robust model can help automate and refine the loan approval process, making it faster and more consistent. It allows for more precise risk assessment, leading to better decisions on who receives a loan and under what terms.\n",
        "4.  **Improved Portfolio Quality:** By selectively approving loans to lower-risk individuals and managing existing loans more effectively, the overall quality of the company's loan portfolio improves, leading to a healthier balance sheet.\n",
        "5.  **Personalized Lending Decisions:** With a deeper understanding of risk factors, the company can offer more tailored loan products, interest rates, and repayment schedules to individual customers, enhancing customer experience and competitive advantage.\n",
        "\n",
        "### Importance of Model Interpretability\n",
        "\n",
        "For a critical application like loan default prediction in a FinTech environment, **model interpretability is paramount**, perhaps as important as predictive accuracy. Understanding *why* a model makes a particular prediction is crucial for several reasons:\n",
        "\n",
        "1.  **Actionable Insights for Business Stakeholders:** Interpretability tools (e.g., SHAP values, feature importance from tree-based models) can reveal the key drivers behind a loan default prediction. For instance, if 'high debt-to-income ratio' and 'inconsistent employment history' are consistently identified as strong indicators for default, the company can refine its lending policies, tighten criteria for these factors, or offer specific support to applicants exhibiting these traits. This moves the model from a black box to a decision-support tool.\n",
        "2.  **Regulatory Compliance:** Financial institutions are often subject to strict regulations (e.g., fair lending laws, explainable AI requirements). Being able to explain why a loan was approved or denied to a customer, or to regulatory bodies, is not just good practice but often a legal necessity. Interpretability provides the transparency needed for compliance.\n",
        "3.  **Trust and Accountability:** Interpretability builds trust among stakeholders and customers. If a model's decision can be explained, it fosters confidence and allows for accountability, enabling human experts to audit and challenge model predictions when necessary.\n",
        "4.  **Model Debugging and Improvement:** Understanding which features influence predictions can help in identifying data quality issues, biases, or areas where the model might be making incorrect associations, thereby guiding further model development and refinement.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "In summary, while a highly accurate and interpretable loan default prediction model offers immense value to a FinTech company through enhanced risk management, financial loss reduction, and strategic decision-making, the **current iteration of our model requires substantial improvement**. The initial performance metrics indicate that it is not yet ready for practical application. Future efforts must focus on exploring more advanced feature engineering, refining the handling of class imbalance, re-tuning hyperparameters more extensively, and potentially experimenting with other sophisticated boosting configurations to achieve a robust and reliable solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46d8b31f"
      },
      "source": [
        "## Business Benefits and Interpretation\n",
        "\n",
        "### Current Model Performance Acknowledgment\n",
        "\n",
        "Based on the calculated evaluation metrics – ROC AUC of 0.5031, PR AUC of 0.2706, F1-score of 0.2238, Precision of 0.2319, and Recall of 0.2162 – it is evident that the **current CatBoost model's performance is not suitable for direct deployment** in a real-world loan default prediction system. An ROC AUC value close to 0.5 indicates that the model is performing no better than random guessing. Similarly, the low F1-score, Precision, and Recall values suggest that the model is not effectively identifying default cases, which is the minority and critical class. This highlights that significant further work is required to improve the model's predictive power.\n",
        "\n",
        "### Potential Business Benefits of a Well-Performing Model\n",
        "\n",
        "Despite the current model's limitations, a **well-performing loan default prediction model** would bring substantial benefits to a FinTech company:\n",
        "\n",
        "1.  **Proactive Risk Management:** Accurately identifying potential defaulters enables the company to take proactive measures, such as offering financial counseling, restructuring loans, or adjusting credit limits, before a default occurs. This moves from reactive loss mitigation to proactive risk management.\n",
        "2.  **Reduced Financial Losses:** By predicting and mitigating defaults, the company can significantly reduce charge-offs and non-performing loans, directly impacting profitability and financial stability.\n",
        "3.  **Optimized Loan Approval Processes:** A robust model can help automate and refine the loan approval process, making it faster and more consistent. It allows for more precise risk assessment, leading to better decisions on who receives a loan and under what terms.\n",
        "4.  **Improved Portfolio Quality:** By selectively approving loans to lower-risk individuals and managing existing loans more effectively, the overall quality of the company's loan portfolio improves, leading to a healthier balance sheet.\n",
        "5.  **Personalized Lending Decisions:** With a deeper understanding of risk factors, the company can offer more tailored loan products, interest rates, and repayment schedules to individual customers, enhancing customer experience and competitive advantage.\n",
        "\n",
        "### Importance of Model Interpretability\n",
        "\n",
        "For a critical application like loan default prediction in a FinTech environment, **model interpretability is paramount**, perhaps as important as predictive accuracy. Understanding *why* a model makes a particular prediction is crucial for several reasons:\n",
        "\n",
        "1.  **Actionable Insights for Business Stakeholders:** Interpretability tools (e.g., SHAP values, feature importance from tree-based models) can reveal the key drivers behind a loan default prediction. For instance, if 'high debt-to-income ratio' and 'inconsistent employment history' are consistently identified as strong indicators for default, the company can refine its lending policies, tighten criteria for these factors, or offer specific support to applicants exhibiting these traits. This moves the model from a black box to a decision-support tool.\n",
        "2.  **Regulatory Compliance:** Financial institutions are often subject to strict regulations (e.g., fair lending laws, explainable AI requirements). Being able to explain why a loan was approved or denied to a customer, or to regulatory bodies, is not just good practice but often a legal necessity. Interpretability provides the transparency needed for compliance.\n",
        "3.  **Trust and Accountability:** Interpretability builds trust among stakeholders and customers. If a model's decision can be explained, it fosters confidence and allows for accountability, enabling human experts to audit and challenge model predictions when necessary.\n",
        "4.  **Model Debugging and Improvement:** Understanding which features influence predictions can help in identifying data quality issues, biases, or areas where the model might be making incorrect associations, thereby guiding further model development and refinement.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "In summary, while a highly accurate and interpretable loan default prediction model offers immense value to a FinTech company through enhanced risk management, financial loss reduction, and strategic decision-making, the **current iteration of our model requires substantial improvement**. The initial performance metrics indicate that it is not yet ready for practical application. Future efforts must focus on exploring more advanced feature engineering, refining the handling of class imbalance, re-tuning hyperparameters more extensively, and potentially experimenting with other sophisticated boosting configurations to achieve a robust and reliable solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d26537de"
      },
      "source": [
        "## Business Benefits and Interpretation\n",
        "\n",
        "### Current Model Performance Acknowledgment\n",
        "\n",
        "Based on the calculated evaluation metrics – ROC AUC of 0.5031, PR AUC of 0.2706, F1-score of 0.2238, Precision of 0.2319, and Recall of 0.2162 – it is evident that the **current CatBoost model's performance is not suitable for direct deployment** in a real-world loan default prediction system. An ROC AUC value close to 0.5 indicates that the model is performing no better than random guessing. Similarly, the low F1-score, Precision, and Recall values suggest that the model is not effectively identifying default cases, which is the minority and critical class. This highlights that significant further work is required to improve the model's predictive power.\n",
        "\n",
        "### Potential Business Benefits of a Well-Performing Model\n",
        "\n",
        "Despite the current model's limitations, a **well-performing loan default prediction model** would bring substantial benefits to a FinTech company:\n",
        "\n",
        "1.  **Proactive Risk Management:** Accurately identifying potential defaulters enables the company to take proactive measures, such as offering financial counseling, restructuring loans, or adjusting credit limits, before a default occurs. This moves from reactive loss mitigation to proactive risk management.\n",
        "2.  **Reduced Financial Losses:** By predicting and mitigating defaults, the company can significantly reduce charge-offs and non-performing loans, directly impacting profitability and financial stability.\n",
        "3.  **Optimized Loan Approval Processes:** A robust model can help automate and refine the loan approval process, making it faster and more consistent. It allows for more precise risk assessment, leading to better decisions on who receives a loan and under what terms.\n",
        "4.  **Improved Portfolio Quality:** By selectively approving loans to lower-risk individuals and managing existing loans more effectively, the overall quality of the company's loan portfolio improves, leading to a healthier balance sheet.\n",
        "5.  **Personalized Lending Decisions:** With a deeper understanding of risk factors, the company can offer more tailored loan products, interest rates, and repayment schedules to individual customers, enhancing customer experience and competitive advantage.\n",
        "\n",
        "### Importance of Model Interpretability\n",
        "\n",
        "For a critical application like loan default prediction in a FinTech environment, **model interpretability is paramount**, perhaps as important as predictive accuracy. Understanding *why* a model makes a particular prediction is crucial for several reasons:\n",
        "\n",
        "1.  **Actionable Insights for Business Stakeholders:** Interpretability tools (e.g., SHAP values, feature importance from tree-based models) can reveal the key drivers behind a loan default prediction. For instance, if 'high debt-to-income ratio' and 'inconsistent employment history' are consistently identified as strong indicators for default, the company can refine its lending policies, tighten criteria for these factors, or offer specific support to applicants exhibiting these traits. This moves the model from a black box to a decision-support tool.\n",
        "2.  **Regulatory Compliance:** Financial institutions are often subject to strict regulations (e.g., fair lending laws, explainable AI requirements). Being able to explain why a loan was approved or denied to a customer, or to regulatory bodies, is not just good practice but often a legal necessity. Interpretability provides the transparency needed for compliance.\n",
        "3.  **Trust and Accountability:** Interpretability builds trust among stakeholders and customers. If a model's decision can be explained, it fosters confidence and allows for accountability, enabling human experts to audit and challenge model predictions when necessary.\n",
        "4.  **Model Debugging and Improvement:** Understanding which features influence predictions can help in identifying data quality issues, biases, or areas where the model might be making incorrect associations, thereby guiding further model development and refinement.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "In summary, while a highly accurate and interpretable loan default prediction model offers immense value to a FinTech company through enhanced risk management, financial loss reduction, and strategic decision-making, the **current iteration of our model requires substantial improvement**. The initial performance metrics indicate that it is not yet ready for practical application. Future efforts must focus on exploring more advanced feature engineering, refining the handling of class imbalance, re-tuning hyperparameters more extensively, and potentially experimenting with other sophisticated boosting configurations to achieve a robust and reliable solution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e7d36f7"
      },
      "source": [
        "## Business Benefits and Interpretation\n",
        "\n",
        "### Current Model Performance Acknowledgment\n",
        "\n",
        "Based on the calculated evaluation metrics – ROC AUC of 0.5031, PR AUC of 0.2706, F1-score of 0.2238, Precision of 0.2319, and Recall of 0.2162 – it is evident that the **current CatBoost model's performance is not suitable for direct deployment** in a real-world loan default prediction system. An ROC AUC value close to 0.5 indicates that the model is performing no better than random guessing. Similarly, the low F1-score, Precision, and Recall values suggest that the model is not effectively identifying default cases, which is the minority and critical class. This highlights that significant further work is required to improve the model's predictive power.\n",
        "\n",
        "### Potential Business Benefits of a Well-Performing Model\n",
        "\n",
        "Despite the current model's limitations, a **well-performing loan default prediction model** would bring substantial benefits to a FinTech company:\n",
        "\n",
        "1.  **Proactive Risk Management:** Accurately identifying potential defaulters enables the company to take proactive measures, such as offering financial counseling, restructuring loans, or adjusting credit limits, before a default occurs. This moves from reactive loss mitigation to proactive risk management.\n",
        "2.  **Reduced Financial Losses:** By predicting and mitigating defaults, the company can significantly reduce charge-offs and non-performing loans, directly impacting profitability and financial stability.\n",
        "3.  **Optimized Loan Approval Processes:** A robust model can help automate and refine the loan approval process, making it faster and more consistent. It allows for more precise risk assessment, leading to better decisions on who receives a loan and under what terms.\n",
        "4.  **Improved Portfolio Quality:** By selectively approving loans to lower-risk individuals and managing existing loans more effectively, the overall quality of the company's loan portfolio improves, leading to a healthier balance sheet.\n",
        "5.  **Personalized Lending Decisions:** With a deeper understanding of risk factors, the company can offer more tailored loan products, interest rates, and repayment schedules to individual customers, enhancing customer experience and competitive advantage.\n",
        "\n",
        "### Importance of Model Interpretability\n",
        "\n",
        "For a critical application like loan default prediction in a FinTech environment, **model interpretability is paramount**, perhaps as important as predictive accuracy. Understanding *why* a model makes a particular prediction is crucial for several reasons:\n",
        "\n",
        "1.  **Actionable Insights for Business Stakeholders:** Interpretability tools (e.g., SHAP values, feature importance from tree-based models) can reveal the key drivers behind a loan default prediction. For instance, if 'high debt-to-income ratio' and 'inconsistent employment history' are consistently identified as strong indicators for default, the company can refine its lending policies, tighten criteria for these factors, or offer specific support to applicants exhibiting these traits. This moves the model from a black box to a decision-support tool.\n",
        "2.  **Regulatory Compliance:** Financial institutions are often subject to strict regulations (e.g., fair lending laws, explainable AI requirements). Being able to explain why a loan was approved or denied to a customer, or to regulatory bodies, is not just good practice but often a legal necessity. Interpretability provides the transparency needed for compliance.\n",
        "3.  **Trust and Accountability:** Interpretability builds trust among stakeholders and customers. If a model's decision can be explained, it fosters confidence and allows for accountability, enabling human experts to audit and challenge model predictions when necessary.\n",
        "4.  **Model Debugging and Improvement:** Understanding which features influence predictions can help in identifying data quality issues, biases, or areas where the model might be making incorrect associations, thereby guiding further model development and refinement.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "In summary, while a highly accurate and interpretable loan default prediction model offers immense value to a FinTech company through enhanced risk management, financial loss reduction, and strategic decision-making, the **current iteration of our model requires substantial improvement**. The initial performance metrics indicate that it is not yet ready for practical application. Future efforts must focus on exploring more advanced feature engineering, refining the handling of class imbalance, re-tuning hyperparameters more extensively, and potentially experimenting with other sophisticated boosting configurations to achieve a robust and reliable solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33d0d153"
      },
      "source": [
        "## Business Benefits and Interpretation\n",
        "\n",
        "### Current Model Performance Acknowledgment\n",
        "\n",
        "Based on the calculated evaluation metrics – ROC AUC of 0.5031, PR AUC of 0.2706, F1-score of 0.2238, Precision of 0.2319, and Recall of 0.2162 – it is evident that the **current CatBoost model's performance is not suitable for direct deployment** in a real-world loan default prediction system. An ROC AUC value close to 0.5 indicates that the model is performing no better than random guessing. Similarly, the low F1-score, Precision, and Recall values suggest that the model is not effectively identifying default cases, which is the minority and critical class. This highlights that significant further work is required to improve the model's predictive power.\n",
        "\n",
        "### Potential Business Benefits of a Well-Performing Model\n",
        "\n",
        "Despite the current model's limitations, a **well-performing loan default prediction model** would bring substantial benefits to a FinTech company:\n",
        "\n",
        "1.  **Proactive Risk Management:** Accurately identifying potential defaulters enables the company to take proactive measures, such as offering financial counseling, restructuring loans, or adjusting credit limits, before a default occurs. This moves from reactive loss mitigation to proactive risk management.\n",
        "2.  **Reduced Financial Losses:** By predicting and mitigating defaults, the company can significantly reduce charge-offs and non-performing loans, directly impacting profitability and financial stability.\n",
        "3.  **Optimized Loan Approval Processes:** A robust model can help automate and refine the loan approval process, making it faster and more consistent. It allows for more precise risk assessment, leading to better decisions on who receives a loan and under what terms.\n",
        "4.  **Improved Portfolio Quality:** By selectively approving loans to lower-risk individuals and managing existing loans more effectively, the overall quality of the company's loan portfolio improves, leading to a healthier balance sheet.\n",
        "5.  **Personalized Lending Decisions:** With a deeper understanding of risk factors, the company can offer more tailored loan products, interest rates, and repayment schedules to individual customers, enhancing customer experience and competitive advantage.\n",
        "\n",
        "### Importance of Model Interpretability\n",
        "\n",
        "For a critical application like loan default prediction in a FinTech environment, **model interpretability is paramount**, perhaps as important as predictive accuracy. Understanding *why* a model makes a particular prediction is crucial for several reasons:\n",
        "\n",
        "1.  **Actionable Insights for Business Stakeholders:** Interpretability tools (e.g., SHAP values, feature importance from tree-based models) can reveal the key drivers behind a loan default prediction. For instance, if 'high debt-to-income ratio' and 'inconsistent employment history' are consistently identified as strong indicators for default, the company can refine its lending policies, tighten criteria for these factors, or offer specific support to applicants exhibiting these traits. This moves the model from a black box to a decision-support tool.\n",
        "2.  **Regulatory Compliance:** Financial institutions are often subject to strict regulations (e.g., fair lending laws, explainable AI requirements). Being able to explain why a loan was approved or denied to a customer, or to regulatory bodies, is not just good practice but often a legal necessity. Interpretability provides the transparency needed for compliance.\n",
        "3.  **Trust and Accountability:** Interpretability builds trust among stakeholders and customers. If a model's decision can be explained, it fosters confidence and allows for accountability, enabling human experts to audit and challenge model predictions when necessary.\n",
        "4.  **Model Debugging and Improvement:** Understanding which features influence predictions can help in identifying data quality issues, biases, or areas where the model might be making incorrect associations, thereby guiding further model development and refinement.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "In summary, while a highly accurate and interpretable loan default prediction model offers immense value to a FinTech company through enhanced risk management, financial loss reduction, and strategic decision-making, the **current iteration of our model requires substantial improvement**. The initial performance metrics indicate that it is not yet ready for practical application. Future efforts must focus on exploring more advanced feature engineering, refining the handling of class imbalance, re-tuning hyperparameters more extensively, and potentially experimenting with other sophisticated boosting configurations to achieve a robust and reliable solution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef81241c"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the described data science pipeline for loan default prediction using boosting techniques, highlighting the key steps from data preprocessing to business impact.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc114f38"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Data Exploration (Conceptual)**: Initial data exploration emphasized the critical need to understand dataset structure, data types, distributions, and the extent/nature of missing values. It highlighted that class imbalance in the target variable (loan default) is crucial to identify early, as it impacts modeling strategy and evaluation.\n",
        "*   **Missing Value Handling**: A conceptual approach was outlined for handling missing values: numerical features primarily imputed with the median, and categorical features with a 'Missing' category (or mode for rare cases). The rationale for each strategy was to preserve information and avoid introducing bias.\n",
        "*   **Categorical Feature Processing**: A synthetic dataset, created to simulate a loan default scenario, featured low-cardinality nominal, high-cardinality nominal, and ordinal features.\n",
        "    *   Missing categorical values in the synthetic data were imputed with 'Missing'.\n",
        "    *   For models like XGBoost/AdaBoost, One-Hot Encoding was chosen for low-cardinality nominal features, Ordinal Encoding for ordinal features (preserving order), and Target Encoding for high-cardinality nominal features (fitted on training data to prevent leakage).\n",
        "    *   For CatBoost, the model was instructed to handle categorical features natively by providing their column names, leveraging its internal Ordered Target Encoding.\n",
        "*   **Class Imbalance**: The synthetic training data exhibited a class imbalance ratio of approximately 3.02:1 (majority class:minority class). The chosen strategy to address this was to incorporate **class weights** (specifically, `scale_pos_weight` for boosting algorithms) rather than using synthetic oversampling techniques like SMOTE, to avoid altering the original data distribution.\n",
        "*   **Model Selection**: **CatBoost** was selected as the most appropriate boosting algorithm, primarily due to its native handling of categorical features (Ordered Target Encoding), its robustness to class imbalance via `class_weights`, and its strong regularization to prevent overfitting.\n",
        "*   **Hyperparameter Tuning**: `GridSearchCV` with `StratifiedKFold` (3 splits) was used to tune CatBoost hyperparameters (`iterations`, `learning_rate`, `depth`, `l2_leaf_reg`, `subsample`). Early stopping (10 rounds based on AUC) and the calculated `scale_pos_weight` were integrated. The optimal hyperparameters identified were `{'depth': 4, 'iterations': 200, 'l2_leaf_reg': 3, 'learning_rate': 0.2, 'subsample': 0.9}`.\n",
        "*   **Model Performance (on Synthetic Data)**: The tuned CatBoost model exhibited very poor performance on the test set of the synthetic data:\n",
        "    *   ROC AUC: 0.5031\n",
        "    *   PR AUC: 0.2706\n",
        "    *   F1-score: 0.2238 (at threshold 0.5)\n",
        "    *   Precision: 0.2319 (at threshold 0.5)\n",
        "    *   Recall: 0.2162 (at threshold 0.5)\n",
        "    This performance indicates the model is barely better than random guessing and is \"not suitable for direct deployment.\"\n",
        "*   **Business Impact**: Acknowledging the current model's poor performance, the analysis outlined significant potential business benefits of a *well-performing* loan default model (e.g., proactive risk management, reduced financial losses). It also emphasized the paramount importance of model interpretability for actionable insights, regulatory compliance, trust, and debugging, which helps move the model from a black box to a decision-support tool.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The current CatBoost model's performance on the synthetic data is insufficient for practical application in a FinTech environment; it requires substantial improvement before deployment.\n",
        "*   Future efforts should focus on more advanced feature engineering, exploring different imputation strategies, refining imbalance handling techniques, performing more extensive hyperparameter tuning (potentially with a wider search space or Bayesian optimization), and experimenting with more complex model architectures to achieve a robust and reliable solution.\n",
        "\n",
        "\n",
        "\n",
        "-------------------------------------------------------------------------------\n"
      ]
    }
  ]
}